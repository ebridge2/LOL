<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<header>
<h1 class="title">Supervised Manifold Learning for Wide Data</h1>
<h2 class="author">Joshua T. Vogelstein, Mauro Maggioni</h2>
</header>
<p>Supervised learning—the art and science of discovering, from data, statistical relationships between multiple different measurement types—is one of the most useful tools in the scientific toolbox. SL has been enabled a wide variety of basic and applied findings, ranging from discovering biomarkers in omics data, to object recognition from images. A special case of SL is classification; a classifier can predict the ’class’ of a novel observation via training on a set of paired observations and class labels (for example, predicting male vs. female from MRI scans). In such problems, the goal is to find the optimal discriminant boundary, which partitions the space of observations into the different classes. In the data age, the ambient (or observed) dimensionality of the observations is quickly ballooning, and with it, the dimensionality of the discriminant boundary. While historical data may have consisted of only a few dimensions (e.g., height and weight), modern scientific datasets often consist of hundreds, thousands, or even millions of dimensions (e.g. genetics, neuroscience, omics). Regardless of the dimensionality, when a scientist or analyst obtains a new dataset consisting of some observations and labels, she must decide which of the myriad available tools to use. Reference algorithms for datasets with low dimensionality include linear and quadratic discriminant analysis, support vector machines, and random forests <span class="citation" data-cites="Hastie2004">Hastie, Tibshirani, and Friedman (<a href="#ref-Hastie2004">2004</a>)</span>.</p>
<p>Classical methods, however, often rely on very restrictive assumptions. In particular, the theoretical guarantees upon which many classical methods rest require that the number of samples is much larger than the dimensionality of the problem (<span class="math inline">\(n \gg p)\)</span>. In scientific contexts, while the dimensionality of datasets is booming, the sample is not witnessing a concomitant increase (see, for example, Table 2 of <span class="citation" data-cites="diMartino2013a">(<span class="citeproc-not-found" data-reference-id="diMartino2013a"><strong>???</strong></span>)</span> in connectomics). When the number of dimensions is orders of magnitude larger than the sample size, as is now typical, this <span class="math inline">\(n \gg p\)</span> assumption is woefully inadequate. This inadequacy is not a mere theoretical footnote; rather, the implementation of the algorithm itself sometimes fails or crashes if this assumption is not met. Worse, often times the algorithm will run to completion, but the answer will be essentially random, with little or no predictive power or accuracy (e.g., <span class="citation" data-cites="Eklund2012">Eklund et al. (<a href="#ref-Eklund2012">2012</a>)</span>).</p>
<p>To combat these issues, the fields of statistics and machine learning have developed a large collection of new methods that relax these assumptions, and exhibit significantly improved performance characteristics. Each such approach makes some “structural” assumptions about the data (sometimes in the form of priors), therefore potentially adding some bias, while reducing variance. The best approach, for a given problem, is the approach that wins this bias/variance trade-off; that is, the approach whose assumptions best correspond to the properties of the data (minimizing bias), and yields estimates have that low error under those assumptions (minimizing variance). These approaches complement “feature engineering” approaches, where the practitioner designs new features based on prior domain specific knowledge.</p>
<p>One of the earliest approaches for discovering low-dimensional representations in supervised learning problems is regularization or shrinkage <span class="citation" data-cites="Friedman1989a Bickel2004a Bouveyron2007">Friedman (<a href="#ref-Friedman1989a">1989</a>; Bickel and Levina <a href="#ref-Bickel2004a">2004</a>; Bouveyron, Girard, and Schmid <a href="#ref-Bouveyron2007">2007</a>)</span>. Many shrinkage methods mitigate the dimensionality problem by smoothing <span class="citation" data-cites="Witten2009a">Witten and Tibshirani (<a href="#ref-Witten2009a">2009</a>)</span>, for example, by regressing parameters to the mean. More recently, a special case of regularization has risen to prominence, called <em>sparsity</em> <span class="citation" data-cites="Olshausen1997a">Olshausen and Field (<a href="#ref-Olshausen1997a">1997</a>)</span>, in which it is assumed that a small number of dimensions can encode the discriminant boundary <span class="citation" data-cites="Tibshirani1996">Tibshirani (<a href="#ref-Tibshirani1996">1996</a>)</span>. This assumption, when accurate, can lead to substantial improvements in accuracy with relatively moderate increase in computational cost <span class="citation" data-cites="Yuan2006a">(<span class="citeproc-not-found" data-reference-id="Yuan2006a"><strong>???</strong></span>)</span>. This framework includes methods such as  <span class="citation" data-cites="Tibshirani1996">Tibshirani (<a href="#ref-Tibshirani1996">1996</a>)</span>, higher criticism thresholding <span class="citation" data-cites="Donoho2008a">Donoho and Jin (<a href="#ref-Donoho2008a">2008</a>)</span>, sure independence screening , distance correlation SIS <span class="citation" data-cites="Li2012">(<span class="citeproc-not-found" data-reference-id="Li2012"><strong>???</strong></span>)</span>, and sparse variants of linear discriminant analysis <span class="citation" data-cites="Tibshirani2002a Fan2008a Witten2009a Clemmensen2011a Mai2013a Fan2012a">Tibshirani et al. (<a href="#ref-Tibshirani2002a">2002</a>; J. Fan and Fan <a href="#ref-Fan2008a">2008</a>; Witten and Tibshirani <a href="#ref-Witten2009a">2009</a>; Clemmensen et al. <a href="#ref-Clemmensen2011a">2011</a>; Mai and Zou <a href="#ref-Mai2013a">2013</a>; J. Fan, Feng, and Tong <a href="#ref-Fan2012a">2012</a>)</span>.</p>
<p>However, for a wide class of problems, such as image classification, sparisty in the ambient space is an overly restrictive, and therefore, bias-inducing assumption (see Figure [fig:mnist] for an example on the classic MNIST dataset). A generalization of sparsity is “low-rank”, in which a small number of linear combinations of the ambient dimensions characterize the data. Unsupervised low-rank methods date back over a century, including multidimensional scaling <span class="citation" data-cites="Householder1938 Borg2005">Householder and Young (<a href="#ref-Householder1938">1938</a>; <span class="citeproc-not-found" data-reference-id="Borg2005"><strong>???</strong></span>)</span> and principal components analysis <span class="citation" data-cites="Pearson1901a Jolliffe2002a">Pearson (<a href="#ref-Pearson1901a">1901</a>; Jolliffe <a href="#ref-Jolliffe2002a">2002</a>)</span>. More recent nonlinear versions of unsupervised dimensionality reduction, or manifold learning, include developments from neural network theory such as self-organizing maps <span class="citation" data-cites="Kohonen1982a">Kohonen (<a href="#ref-Kohonen1982a">1982</a>)</span>, generative topographic mapping <span class="citation" data-cites="Bishop1998a">Christopher M. Bishop, Svensén, and Williams (<a href="#ref-Bishop1998a">1998</a>)</span>. In this century, manifold learning became more popular, including isomap <span class="citation" data-cites="Tenenbaum2000a">Tenenbaum et al. (<a href="#ref-Tenenbaum2000a">2000</a>)</span>, local linear embedding <span class="citation" data-cites="Roweis2000a">Roweis and Saul (<a href="#ref-Roweis2000a">2000</a>)</span>, Laplacian eigenmaps <span class="citation" data-cites="Belkin2003a">Belkin and Niyogi (<a href="#ref-Belkin2003a">2003</a>)</span>, local tangent space alignment <span class="citation" data-cites="Zhang2004f">Z. Zhang and Zha (<a href="#ref-Zhang2004f">2004</a>)</span>, diffusion maps <span class="citation" data-cites="Coifman2006a">Coifman and Lafon (<a href="#ref-Coifman2006a">2006</a>)</span>, and geometric multi-resolution analysis <span class="citation" data-cites="Allard2012">Allard, Chen, and Maggioni (<a href="#ref-Allard2012">2012</a>)</span>. All these approaches can be used as pre-processing steps, to reduce the dimensionality of the data prior to solving the supervised learning problem <span class="citation" data-cites="Belhumeur1997a">Belhumeur, Hespanha, and Kriegman (<a href="#ref-Belhumeur1997a">1997</a>)</span>.</p>
<p>However, such manifold learning methods, while exhibiting both strong theoretical <span class="citation" data-cites="Eckart1936a deSilva2003 Allard2012">Eckart and Young (<a href="#ref-Eckart1936a">1936</a>; Silva and Tenenbaum <a href="#ref-deSilva2003">2003</a>; Allard, Chen, and Maggioni <a href="#ref-Allard2012">2012</a>)</span> and empirical performance, are fully unsupervised. Thus, in classification problems, they discover a low-dimensional representation of the data, ignoring the labels. This can be highly problematic when the discriminant dimensions and the directions of maximal variance are not aligned (see Figure [fig:mnist] for an example). Supervised dimensionality reduction techniques, therefore, combine the best of both worlds, search for low-dimensional discriminant boundaries. A set of methods from the statistics community is collectively referred to as “sufficient dimensionality reduction” (SIR) or “first two moments” (F2M) methods <span class="citation" data-cites="Li1991a Tishby1999a Globerson2003a Cook2005a Fukumizu2004a">K.-C. Li (<a href="#ref-Li1991a">1991</a>; Tishby, Pereira, and Bialek <a href="#ref-Tishby1999a">1999</a>; Globerson and Tishby <a href="#ref-Globerson2003a">2003</a>; Cook and Ni <a href="#ref-Cook2005a">2005</a>; Fukumizu, Bach, and Jordan <a href="#ref-Fukumizu2004a">2004</a>)</span>. These methods are theoretically elegant, but typically require the sample size to be larger than the number of observed dimensions (although see <span class="citation" data-cites="Cook2013">Cook, Forzani, and Rothman (<a href="#ref-Cook2013">2013</a>)</span> for some promising work). Other approaches formulate an optimization problem, such as projection pursuit <span class="citation" data-cites="Huber1985a">Huber (<a href="#ref-Huber1985a">1985</a>)</span>, empirical risk minimization <span class="citation" data-cites="Belkin2006a">Belkin, Niyogi, and Sindhwani (<a href="#ref-Belkin2006a">2006</a>)</span>, or supervised dictionary learning <span class="citation" data-cites="Mairal2009">Mairal et al. (<a href="#ref-Mairal2009">2009</a>)</span>. These methods are limited because they are prone to fall into local minima, they require costly iterative algorithms, and lack any theoretical guarantees <span class="citation" data-cites="Belkin2006a">Belkin, Niyogi, and Sindhwani (<a href="#ref-Belkin2006a">2006</a>)</span>. Thus, there remains a gap in the literature: a supervised learning method with theoretical convergence guarantees appropriate when the dimensionality is orders of magnitude larger than the sample size.</p>
<p>The challenge lies is posing the problem in such a way that efficient numerical algorithms can be brought to bear, without costly iterations or tuning parameters. Our approach, which we call “Linear Optimal Low-rank” () embedding (see Figure [fig:mnist]), utilizes the first two moments, as do SIR, spectral decompositions, and high-dimensional discriminant analysis methods <span class="citation" data-cites="Bouveyron2007">Bouveyron, Girard, and Schmid (<a href="#ref-Bouveyron2007">2007</a>)</span>, but does not require iterative algorithms and therefore is vastly more computationally efficient. The motivation for  comes from a simple geometric intuition (Figure [fig:cigars]). Indeed, we provide theoretical insight explaining why our method is more general than previous approaches. A variety of simulations provide further evidence that  efficiently finds a better low-dimensional representation than competing methods, not just under the provable model assumptions, but also under much more general contexts (Figure [fig:properties]). Moreover, we demonstrate that  achieves better performance, in less time, as compared to several reference high-dimensional classifiers, on several benchmark datasets, including genomics, connectomics, and image processing problems (Figure [fig:realdata]). Finally,  can also be used to improve high-dimensional regression and testing (Figure [fig:generalizations]). Based on the above, we suggest that   be considered as one of the reference method for supervised manifold learning for wide data. For reproducibility and extensibility, MATLAB code to run all numerical experiments and reproduce all figures is available from our github repository available here: <a href="http://github.com/jovo/lol" class="uri">http://github.com/jovo/lol</a>.</p>
<h1>Results</h1>
<h2>An Illustrative Real Data Example of Supervised Linear Manifold Learning</h2>
<figure>
<embed src="../Figs/mnist.pdf" /><figcaption>Illustrating three different classifiers— (top),  (middle), and  (bottom)—for embedding images of the digits 3, 7, and 8 (from MNIST), each of which is 28 <span class="math inline">\(\times\)</span> 28 = 784 dimensional. <strong>(A)</strong>: Exemplars, boundary colors are only for visualization purposes. <strong>(B)</strong>: The first four projection matrices learned by the three different approaches on 300 training samples. Note that  is sparse and supervised,  is dense and unsupervised, and  is dense and supervised. <strong>(C)</strong>: Embedding 500 test samples into the top 2 dimensions using each approach. Digits color coded as in (A). <strong>(D)</strong>: The estimated posterior distribution of test samples after 5-dimensional projection learned via each method. We show only 3 vs. 8 for simplicity. The vertical line shows the classification threshold. The filled area is the estimated error rate: the goal of any classification algorithm is to minimize that area. Clearly,  exhibits the best separation after embedding, which results in the best classification performance.</figcaption>
</figure>
<p>Pseudocode of any method that embeds high-dimensional data as part of classification proceeds as schematized in Figure [fig:mnist]: (A) obtain/select n training samples of the data, (B) learn a low dimensional projection, (C) project n’ testing samples onto the lower dimensional space, (D) classify the embedded testing samples using some classifier. We consider three different linear dimensionality reduction methods—, , and —each of which we compose with a classifier to form high-dimensional classifiers.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>To demonstrate the utility of , we first consider one of the most popular benchmark datasets ever, the MNIST dataset <span class="citation" data-cites="mnist">LeCun, Cortes, and Burges (<a href="#ref-mnist">2015</a>)</span>. This dataset consists of many thousands of examples of images of the digits 0 through 9. Each such image is represented by a 28$$28 matrix, which means that the observed (or ambient) dimensionality of the data is $p=$784. Because we are motivated by the <span class="math inline">\(n \ll p\)</span> scenario, we subsample the data to select n=300 examples of the numbers 3, 7, and 8. We then apply all three approaches to this subsample of the MNIST dataset, learning a projection, and embedding n’=500 testing samples, and classifying the resulting embedded data.</p>
<p>, by virtue of being a sparse method, finds the pixels that most discriminate the 3 classes. The resulting embeddings mostly live along the boundaries, because these images are close to binary, and therefore, images either have or do not have a particular pixel. Indeed, although the images themselves are nearly sparse (over 80% of the pixels in the dataset have intensity <span class="math inline">\(\leq 0.05\)</span>), a low-dimensional discriminant boundary does not seem to be so. , on the other hand, finds the linear combinations of training samples that maximize the variance. This unsupervised linear manifold learning method results in projection matrices that indeed look like linear combinations of the three different digits. The goal here, however, is separating classes, not maximizing variability. The resulting embeddings are not particularly well separated, suggesting the the directions of discriminability are not the same as the directions of maximum variance.  is our newly proposed supervised linear manifold learning method (see below for details). The projection matrices it learns look qualitatively much like those of . This is not surprising, as both are linear combinations of the training examples. The resulting embeddings however, look quite different. The three different classes are very clearly separated by even the first two dimensions. The result of these embeddings yields classifiers whose performance is obvious from looking at the embeddings:  achieves significantly smaller error than the other two approaches. This numerical experiment justifies the use of supervised linear manifold learning, we next investigate the performance of these methods in simpler simulated examples, to better illustrate when we can expect  to outperform other methods, and perhaps more importantly, when we expect this “vanilla” variant of  to fail.</p>
<h2>Linear Gaussian Intuition</h2>
<p>The above real data example suggests the geometric intuition for when  outperforms its sparse and unsupervised counterparts. To further investigate, both theoretically and numerically, we consider the simplest setting that illustrates the relevant geometry. In particular, we consider a two-class classification problem, where both classes are distributed according to a multivariate normal distribution, the class priors are equal, and the joint distribution is centered, so that the only difference between the classes is their means (we call this the Linear Discriminant Analysis () model; see Methods for details).</p>
<p>To motivate , and the following simulations, lets consider what the optimal projection would be in this scenario. The optimal low-dimensional projection is analytically available as the dot product of the difference of means and the inverse covariance matrix, <span class="math inline">\(\mb{A}_*=\mb{\delta}\T \bSig^{-1}\)</span> <span class="citation" data-cites="Bickel2004a">Bickel and Levina (<a href="#ref-Bickel2004a">2004</a>)</span> (see Methods for details). , the dominant unsupervised manifold learning method, utilizes only the covariance structure of the data, and ignores the difference between the means. In particular,  would project the data on the top d eigenvectors of the covariance matrix. <strong>The key insight of our work is the following: we can use both the difference of the means and the covariance matrix, rather than just the covariance matrix, to find a low dimensional projection.</strong> Naively, this should typically improve performance, because in this stylized scenario, both are important. Formally, we implement this idea by simply concatenating the difference of the means with the top d eigenvectors of the covariance. This is equivalent to first projecting onto the difference of the means vector, and then projecting the residuals onto the first d principle components. Thus, it requires almost no additional computational time or complexity, rather, merely estimates the difference of the means. In this sense,  can be thought of as a very simply “supervised ”.</p>
<figure>
<embed src="../Figs/cigars_est.pdf" /><figcaption> achieves near optimal performance for a wide variety of distributions. Each point is sampled from a multivariate Gaussian; the three columns correspond to different simulation parameters (see Methods for details). In each of 3 simulations, we sample n=100 points in p=1000 dimensions. And for each approach, we embed into the top 20 dimensions. Note that we use the sample estimates, rather than the true population values of the parameters. In this setting, the results are similar. <strong>(A)</strong>: The mean difference vector is aligned with the direction of maximal variance, maxing it ideal for both  to discover the discriminant direction and a sparse solution. <strong>(B)</strong>: The mean difference vector is orthogonal to the direction of maximal variance, making  fail, but sparse methods can still recover the correct dimensions. <strong>(C)</strong>: Same as (B), but the data are rotated. <strong>Row 1</strong>: A scatter plot of the first two dimensions of the sampled points, with class 0 and 1 as black and gray dots, respectively. <strong>Row 2</strong>: . <strong>Row 3</strong>: , a sparse method designed specifically for this model <span class="citation" data-cites="Fan2012a">J. Fan, Feng, and Tong (<a href="#ref-Fan2012a">2012</a>)</span>. <strong>Row 4</strong>: , our newly proposed method. <strong>Row 5</strong>: the Bayes optimal classifier, which is what all classifiers strive to achieve. Note that  is closest to Bayes optimal in all three settings.</figcaption>
</figure>
<p>Figure [fig:cigars] shows three different examples of data sampled from the  model to geometrically illustration this intuition. In each, we sample n=100 training samples in p=1000 dimensional space, so <span class="math inline">\(n \ll p\)</span>. Figure <a href="A">fig:cigars</a> shows an example we call “stacked cigars”. In this example and the next, the covariance matrix is diagonal, so all ambient dimensions are independent of one another. Moreover, the difference between the means and diagonal are both large along the same dimensions (they are highly correlated with one another). This is an idealized setting for , because  finds the direction of maximal variance, which happens to correspond to the direction of maximal separation. However,  does not weight the discriminant directions sufficiently, and therefore performs only moderately well.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> Because all dimensions are independent, this is a good scenario for sparse methods. Indeed, , a sparse classifier designed for precisely this scenario, does an excellent job finding the most useful ambient dimensions.  does the best of all three approaches, by using both the difference of the means and the covariance.</p>
<p>Figure <a href="B">fig:cigars</a> shows an example which is a worst case scenario for using  to find the optimal projection for classification. In particular, the variance is getting larger for subsequent dimensions, <span class="math inline">\(\sigma_1 &lt; \sigma_2 &lt; \cdots &lt; \sigma_p\)</span>, while the magnitudes of the difference between the means are decreasing with dimension, <span class="math inline">\(\delta_1 &gt; \delta_2 &lt; \cdots &gt; \delta_p\)</span>. Thus, for any truncation level,  finds exactly the <em>wrong</em> directions. is not hampered by this problem, it is also able to find the directions of maximal discrimination, rather than those of maximal variance. Again, , by using both parameters, does extremely well.</p>
<p>Figure <a href="C">fig:cigars</a> is exactly the same as (B), except the data have been randomly rotated in all 1000 dimensions. This means that none of the original coordinates have much information, rather, linear combinations of them do. This is evidenced by observing the scatter plot, which shows that two dimensions clearly fail to disambiguate the two classes. , being rotationally invariant, fails in this scenario as it did in (B). Now, there is no small number of ambient dimensions that separate the data well, so also fails. However, , by virtue of being rotationally invariant, is unperturbed by this rotation. In particular, it is able to “unrotate” the data, to find dimensions that optimally separate the two classes.</p>
<h2>Theoretical Confirmation</h2>
<p>The above numerical experiments provide the intuition to guide our theoretical developments.</p>
<p>[thm:LDA] Under the  model,  is better than .</p>
<p>In words, it is better to incorporate the mean difference vector into the projection matrix. The degree of improvement is a function of the embedding dimension d, the ambient dimensionality p, and the parameters (see Methods for details and proof).</p>
<h2>How many dimensions to keep?</h2>
<p>In the above numerical and theoretical investigations, we fixed d, the number of dimensions to embed into. Much unsupervised manifold learning theory typically focuses on finding the “true” intrinsic dimensionality of the data. The analogous question for supervised manifold learning would be to find the true intrinsic dimensionality of the discriminant boundary. However, in real data problems, typically, their is no perfect low dimensional representation. Thus, in all the following simulations, the true ambient dimensionality of the data is equal to the dimensionality of the optimal discriminant boundary (given infinite data). In other words, there does not exist a discriminant space that is lower dimensional than the ambient space, so we cannot find the “intrinsic dimension” of the data or the discriminant boundary. Rather, we face a trade-off: keeping more dimensions reduces bias, but increases variance. The optimal bias/variance trade-off depends on the distribution of the data, as well as the sample size <span class="citation" data-cites="Trunk1979a">Trunk (<a href="#ref-Trunk1979a">1979</a>)</span>. We formalize this notion for the LpA model and proof the following:</p>
<p>[thm:n] Under the   model, estimated  is better than .</p>
<p>Note that the degree of improvement is a function of the number of samples n, in addition to the embedding dimension d, the ambient dimensionality p, and the parameters (see Methods for details and proof).</p>
<p>Consider again the rotated trunk example as well as a “Toeplitz” example, as depicted in Figures <a href="A">fig:properties</a> and (B). In both cases, the data are sampled from the   model, and in both cases, the optimal dimensionality depends on the particular approach, but is never the true dimensionality. Moreover,  dominates the other approaches, regardless of the number of dimensions used. Figure <a href="C">fig:properties</a> shows a sparse example with “fat tails” to mirror real data settings better. The qualitative results are consistent with those of (A) and (B). Indeed, we can generalize Theorem [thm:n] to include “sub-Gaussian” data, rather than just Gaussian:</p>
<p>[thm:FAT] Under a sub-Gaussian generalization of the   model,  is still better than .</p>
<h2>Multiple Classes</h2>
<p> can trivially be extended to <span class="math inline">\(&gt;2\)</span> class situations. Naively it may seem like we would need to keep all pairwise differences between means. However, given <span class="math inline">\(k\)</span> classes, the set of all <span class="math inline">\(k^2\)</span> differences is only rank <span class="math inline">\(k-1\)</span>. In other words, we can equivalently find the class which has the maximum number of samples (breaking ties randomly), and subtract its mean from all other class means. Figure <a href="D">fig:properties</a> shows a 3-class generalization of (A). While  uses the additional class naturally, many previously proposed high-dimensional  variants, such as , natively only work for 2-classes.</p>
<figure>
<embed src="../Figs/properties.pdf" /><figcaption>Seven simulations demonstrating that even when the true discriminant boundary is high-dimensional,  can find a low-dimensional projection that wins the bias-variance trade-off against competing methods. For the first four, the top panels depict the means (top), the shared covariance matrix (middle). For the next three, the top panels depict a 2D scatter plot (left), mean and level set of one standard deviation of covariance matrix (right). For all seven simulations, the bottom panel shows misclassification rate as a function of the number of embedded dimensions, for several different classifiers. The simulations settings are as follows: <strong>(A)</strong> Rotated Trunk: same as Figure <a href="C">fig:cigars</a>. <strong>(B)</strong> Toeplitz: another setting where mean difference is not well correlated with any eigenvector, and no ambient coordinate is particularly useful on its own. <strong>(C)</strong> Fat Tails: a common phenomenon in real data; we have theory to support this generalization of the   model. <strong>(D)</strong> 3 Classes:  naturally adapts to multiple classes. <strong>(E)</strong> QDA: QOQ, a variant of  when each class has a unique covariance, outperforms , as expected. <strong>(F)</strong> Outliers: adding high-dimensional outliers degrades performance of standard eigensolvers, but those can easily be replaced in  for a robust variants (called ). <strong>(F)</strong> XOR: a high-dimensional stochastic generalization of XOR, demonstrating the  and QOQ work even in scenarios that are quite distinct from the original motivating problems. In all 7 cases, , or the appropriate generalization thereof, outperforms unsupervised, sparse, or other methods. Moreover, the optimal embedding dimension is never the true discriminant dimension, but rather, a smaller number jointly determined by parameter settings and sample size.</figcaption>
</figure>
<h2>Generalizations of</h2>
<p>The simple geometric intuition which led to the development of  suggests that we can easily generalize  to be more appropriate for more complicated settings. We consider three additional scenarios:</p>
<p>Sometimes, it makes more sense to model each class as having a unique covariance matrix, rather than a shared covariance matrix. Assuming everything is Gaussian, the optimal classifier in this scenario is called Quadratic Discriminant Analysis (QDA) <span class="citation" data-cites="Hastie2004">Hastie, Tibshirani, and Friedman (<a href="#ref-Hastie2004">2004</a>)</span>. Intuitively then, we can modify  to compute the eigenvectors separately for each class, and concatenate them (sorting them according to their singular values). Moreover, rather than classifying the projected data with , we can then classify the projected data with QDA. Indeed, simulating data according to such a model (Figure <a href="E">fig:properties</a>,  performs slightly above chance, regardless of the number of dimensions we use to project, whereas QOQ (which denotes we estimate eigenvectors separately and then use QDA on the projected data) performs significantly better regardless of how many dimensions it keeps.</p>
<p>Outliers persist in many real data sets. Finding outliers, especially in high-dimensional data, is both tedious and difficult. Therefore, it is often advantageous to have estimators that are robust to certain kinds of outliers <span class="citation" data-cites="Huber1981a Rousseeuw1999a Ferrari2010a">Huber (<a href="#ref-Huber1981a">1981</a>; Rousseeuw and Driessen <a href="#ref-Rousseeuw1999a">1999</a>; <span class="citeproc-not-found" data-reference-id="Ferrari2010a"><strong>???</strong></span>)</span>.  and eigenvector computation are particularly sensitive to outliers <span class="citation" data-cites="Candes2009b">Candès et al. (<a href="#ref-Candes2009b">2009</a>)</span>. Because  is so simple and modular, we can replace typical eigenvector computation with a robust variant thereof, such as <span class="citation" data-cites="Zhang2014a">T. Zhang and Lerman (<a href="#ref-Zhang2014a">2014</a>)</span>. Figure <a href="F">fig:properties</a> shows an example where we generated <span class="math inline">\(n/2\)</span> training samples according to the simple   model, but then added another <span class="math inline">\(n/2\)</span> training samples from a noise model.  (our robust variant of  that simply replaces the fragile eigenvector computation with a robust version), performs better than  regardless of the number of dimensions we keep.</p>
<p>XOR is perhaps the simplest nonlinear problem, the problem that led to the demise of the perceptron, prior to its resurgence after the development of multi-layer perceptrons <span class="citation" data-cites="Bishop2006a">Christopher M Bishop (<a href="#ref-Bishop2006a">2006</a>)</span>. Thus, in our opinion, it is warranted to check whether any new classification method can perform well in this scenario. The classical (two-dimensional) XOR problem is quite simple: the output of a classifier is zero if both inputs are the same (00 or 11), and the output is one if the inputs differ (01 or 10). Figure <a href="G">fig:properties</a> shows a high dimensional and stochastic variant of XOR. This simulation was designed such that standard classifiers, such as support vector machines and random forests, achieve chance levels (not shown). , performs moderately better than chance, and QOQ performs significantly better than chance, regardless of the chosen dimensionality. This demonstrates that our classifiers developed herein, though quite simple and intuition, can perform well even in settings where the data are badly modeled by our underlying assumptions. This mirrors previous findings where the so-called “idiots’s Bayes” classifier outperforms more sophisticated classifiers <span class="citation" data-cites="Bickel2004a">Bickel and Levina (<a href="#ref-Bickel2004a">2004</a>)</span>. In fact, we think of our work as finding intermediate points between idiot’s Bayes (or naive Bayes) and , by enabling degrees of regularization by changing the dimensionality used.</p>
<h2>Computational Efficiency</h2>
<p>In many applications, the main quantifiable consideration in whether to use a particular method, other than accuracy, is numerical efficiency. Because implementing  requires only highly optimized linear algebraic routines—including computing moments and singular value decomposition—rather than the costly iterative programming techniques currently required for sparse or dictionary learning type problems. To quantify the computational efficiency of  and its variants, Figure [fig:speed] shows the wall time it takes to run each method on the stacked cigars problem, varying the ambient dimensionality, embedded dimensionality, and sample size. Note that for completeness, we include two additional variants of :  and .  (short for fast ) replaces the standard  algorithm with a randomized variant, which can be much faster in certain situations <span class="citation" data-cites="Halko2011a">Halko, Martinsson, and Tropp (<a href="#ref-Halko2011a">2011</a>)</span>.  goes even one step further, replacing  with random projections <span class="citation" data-cites="Candes2006a">Candès (<a href="#ref-Candes2006a">2006</a>)</span>. This variant of  is the fastest, its runtime is least sensitive to (p,d,n), and its accuracy is often commensurate (or better) than other variants of . We will explore Ra in future work. Note that the runtime of all the variants of  are quite similar to . Given, given ’s improved accuracy, and nearly identical simplicity, it seems there is very little reason to not use  instead of .</p>
<figure>
<embed src="../Figs/speed_test.pdf" /><figcaption>Computational efficiency of various low-dimensional projection methods. In all cases, n=100, and we used the “stacked cigars” simulation parameters. We compare  with the projection steps from , QOQ, , , and , for different values of (p,d). The addition of the mean difference vector is essentially negligible. Moreover, for small d, the  is advantageous.  is always fastest, and its performance is often comparable to other methods (not shown).</figcaption>
</figure>
<h2>Benchmark Real Data Applications</h2>
<p>To more comprehensively understand the relative advantages and disadvantages of  with respect to other high-dimensional classification approaches, in addition to evaluating its performance in theory, and in a variety of numerical simulations, it is important to evaluate it also on benchmark datasets. For these purposes, we have selected four commonly used high-dimensional datasets (see Methods for details). For each, we compare  to (i) support vector machines, (ii) , (iii) lasso, (iv) and random forest (RF). Because in practice all these approaches have “hyperparameters” to tune, we consider several possible values for SVM, lasso, and  (but not RF, as its runtime was too high). Figure [fig:realdata] shows the results for all four datasets.</p>
<p>Qualitatively, the results are similar across datasets:  achieves high accuracy and computational efficiency as compared to the other methodologies. Considering Figure <a href="A">fig:realdata</a> and (B), two popular sparse settings, we find that  can find very low dimensional projections with very good accuracy. For the prostate data, with a sufficiently non-sparse solution for , it slightly outperforms , but at substantial computational cost, in particular, takes about 100 times longer to run on this dataset. Figure <a href="C">fig:realdata</a> and (D) are 10-class problems, so is no longer possible. Here, SVM can again slightly outperform , but again, requiring 100 fold additional computational time. In all cases, the beloved random forest classifier performs subpar.</p>
<figure>
<embed src="../Figs/realdata.pdf" /><figcaption>For four standard datasets, we benchmark  (green circles) versus standard classification methods, including support vector machines (blue up triangles), (cyan down triangles),  (magenta pluses), and random forest (orange diamonds). Top panels show error rate as a function of log<span class="math inline">\(_2\)</span> number of embedded dimensions (for , , and ) or cost (for SVM). Bottom panels show the minimum error rate achieved by each of the five algorithms versus time. The lower left dark gray (upper right light gray) rectangle is the area in which any algorithm is <em>better</em> (worse) than  in terms of both accuracy and efficiency. <strong>(A)</strong> Prostate: a standard sparse dataset. 1-dimensional  does very well, although keeping <span class="math inline">\(2^5\)</span> ambient coordinates slightly improves performance, at a significant cost of compute time (two orders of magnitude), with minimal additional interpretability. <strong>(B)</strong> Colon: another standard sparse dataset. Here, 2-4 dimensions of  outperforms all other approaches considered. <strong>(C)</strong> MNIST: 10 image categories here, so is not possible.  does very well regardless of the number of dimensions kept. SVN marginally improves on  accuracy, at a significant cost in computation (two orders of magnitude). <strong>(D)</strong> CIFAR-10: a higher dimensional and newer 10 category image classification problem. Results are qualitatively similar to (C). Note that, for none of the problems is there an algorithm ever performing better and faster than ; rather, most algorithms typically perform worse and slower (though some are more accurate and much more computationally expensive. This suggests that regardless of how one subjectively weights computational efficiency versus accuracy,  is the best default algorithm in a variety of real data settings.</figcaption>
</figure>
<h2>Extensions to Other Supervised Learning Problems</h2>
<p>The utility of incorporating the mean difference vector into supervised machine learning for wide data extends beyond merely classification. In particular, hypothesis testing can be considered as a special case of classification, with a particular loss function. Therefore we apply the same idea to a hypothesis testing scenario. The multivariate generalization of the t-test, called Hotelling’s Test, suffers from the same problem as does the classification problem; namely, it requires inverting an estimate of the covariance matrix. To mitigate this issue in the hypothesis testing scenario, authors have applied similar tricks as they have done in the classification setting. One particularly nice and related example is that of Lopes et al. <span class="citation" data-cites="Lopes2011a">Lopes, Jacob, and Wainwright (<a href="#ref-Lopes2011a">2011</a>)</span>, who addresses this dilemma by using random projections to obtain a low-dimensional representation, following by applying Hotelling’s Test in the lower dimensional subspace. Figure <a href="A">fig:generalizations</a> and (B) shows the power of their test alongside the power of the same approach, but using the  projection rather than random projections. The two different simulations include the simulated settings considered in their manuscript (see Methods for details). The results make it clear that the  test has higher power for essentially all scenarios. Moreover, it is not merely the replacing random projections with , nor simply incorporating the mean difference vector, but rather, it appears that  for testing uses both modifications to improve performance.</p>
<p>High-dimensional linear regression is another supervised learning method that can utilize this idea. Linear regression, like classification and Hotelling’s Test, requires inverting a singular matrix as well. By projecting the data only a lower dimensional subspace first, followed by linear regression on the low-dimensional data, we can mitigate the curse of high-dimensions. To choose the projection matrix, we partition the data into K partitions, based on the percentile of the target variable, we obtain a K class classification problem. Then, we can apply  to learn the embedding. Figure <a href="C">fig:generalizations</a> shows an example of this approach, contrasted with lasso and partial least squares, in a sparse simulation setting (see Methods for details).  is able to find a better low-dimensional projection than lasso, and performs significantly better than PLS, for essentially all choices of number of dimensions to embed into.</p>
<figure>
<embed src="../Figs/regression_power.pdf" /><figcaption>The intuition of including the mean difference vector is equally useful for other supervised manifold learning problems, including testing and regression. (A) and (B) show two different high-dimensional testing settings, as described in Methods. Power is plotted against the decay rate of the spectrum, which approximates the effective number of dimensions.  composed with Hotelling’s test outperforms the random projections variants described in <span class="citation" data-cites="Lopes2011a">Lopes, Jacob, and Wainwright (<a href="#ref-Lopes2011a">2011</a>)</span>, as well as several other variants. (C) shows a high-dimensional regression settings, as described in Methods. Log<span class="math inline">\(_{10}\)</span> mean squared error is plotted against the number of embedded dimensions. Regression  composed with linear regression outperforms  (cyan), the classic sparse regression method, as well as partial least squares (PLS; black). In the legend, ’A’ denote either ’linear regression’ (in (C)), or ’Hotelling’ (in (A) and (B)). These three simulation settings therefore demonstrate the generality of this technique.</figcaption>
</figure>
<h1>Discussion</h1>
<p>We have introduced a very simple, yet new, device to improve performance on supervised learning problems with wide data. In particular, we have proposed a supervised manifold learning procedure, the utilizes both the difference of the means, and the covariance matrices. This is in stark contrast to previous approaches, which only utilize the covariance matrices (or kernel variants thereof), or solve a difficult optimization theoretic problem. In addition to demonstrating the accuracy and numerical efficiency of  on simulated and real classification problems, we also demonstrate how the same idea can also be used for other kinds of supervised learning problems, including regression and hypothesis testing.</p>
<p>One of the first publications to compose  with an unsupervised learning method was the celebrated Fisherfaces paper <span class="citation" data-cites="Belhumeur1997a">Belhumeur, Hespanha, and Kriegman (<a href="#ref-Belhumeur1997a">1997</a>)</span>. The authors showed via a sequence of numerical experiments the utility of embedding with  prior to classifying with . We extend this work by adding a supervised component to the initial embedding. Moreover, we provide the geometric intuition for why and when this is advantageous, as well as show numerous examples demonstrating its superiority. Finally, we have matrix concentration inequalities proving the advantages of  over Fisherfaces.</p>
<p>The  idea, appending the mean difference vector to convert unsupervised manifold learning to supervised manifold learning, has many potential applications. We have presented the first few. Incorporating additional nonlinearities via kernel methods <span class="citation" data-cites="Mika1999a">Mika et al. (<a href="#ref-Mika1999a">1999</a>)</span>, ensemble methods such as random forests <span class="citation" data-cites="Breiman2001a">Breiman (<a href="#ref-Breiman2001a">2001</a>)</span>, multiscale methods <span class="citation" data-cites="Allard2012">Allard, Chen, and Maggioni (<a href="#ref-Allard2012">2012</a>)</span>, and more scalable implementations <span class="citation" data-cites="Chang2011a">Chang and Lin (<a href="#ref-Chang2011a">2011</a>)</span>, are all of immediate interest.</p>
<figure>
<embed src="../Figs/table.pdf" /><figcaption>Table of algorithms and their properties for high-dimensional data. Gray elements indicate that results are demonstrated in the Figure labeled in the bottom row. ’X’ denotes relatively good performance for a given setting, or has the particular property.</figcaption>
</figure>
<h1>Theoretical Background</h1>
<h2>The Classification Problem</h2>
<p>Let <span class="math inline">\((\bX,Y)\)</span> be a pair of random variables, jointly sampled from <span class="math inline">\(F :=F_{\bX,Y}=F_{\bX|Y}F_{Y}\)</span>. Let <span class="math inline">\(\bX\)</span> be a multivariate vector-valued random variable, such that its realizations live in p dimensional Euclidean space, <span class="math inline">\(\bx \in \Real^p\)</span>. Let <span class="math inline">\(Y\)</span> be a categorical random variable, whose realizations are discrete, <span class="math inline">\(y \in \{0,1,\ldots C\}\)</span>. The goal of a classification problem is to find a function <span class="math inline">\(g(\bx)\)</span> such that its output tends to be the true class label <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
 %\label{eq:bayes}
g^*(\bx) := \argmax_{g \in \mc{G}} \PP[g(\bx) = y].\end{aligned}
\]</span></p>
<p>When the joint distribution of the data is known, then the Bayes optimal solution is:</p>
<p><span class="math display">\[
\begin{aligned}
  \label{eq:R}
g^*(\bx) := \argmax_y f_{y|\bx} = \argmax_y f_{\bx|y}f_y =\argmax_y \{\log f_{\bx|y} + \log f_y \}\end{aligned}
\]</span></p>
<p>Denote expected misclassification rate of classifier <span class="math inline">\(g\)</span> for a given joint distribution <span class="math inline">\(F\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
L^F_g := \EE[g(\bx) \neq y] := \int \PP[g(\bx) \neq y] f_{\bx,y} d\bx dy,\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\EE\)</span> is the expectation, which in this case, is with respect to <span class="math inline">\(F_{XY}\)</span>. For brevity, we often simply write <span class="math inline">\(L_g\)</span>, and we define <span class="math inline">\(L_* := L_{g^*}\)</span>.</p>
<h2>Linear Discriminant Analysis ()</h2>
<p>Linear Discriminant Analysis () is an approach to classification that uses a linear function of the first two moments of the distribution of the data. More specifically, let <span class="math inline">\(\mu_j=\EE[F_{X|Y=j}]\)</span> denote the class conditional mean, and let <span class="math inline">\(\bSig=\EE[F_{X}^2]\)</span> denote the joint covariance matrix, and <span class="math inline">\(\pi_j=\PP[Y=j]\)</span>. Using this notation, we can define the  classifier:</p>
<p><span class="math display">\[
\begin{aligned}
g_{\Lda}(\bx)&amp;:=\argmin_y \frac{1}{2} (\bx-\bmu_0)\T \bSig^{-1}(\bx-\bmu_0) + \II\{Y=y\}  \log \pi_y,\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\II\{ \cdot\}\)</span> is one when its argument is true, and zero otherwise. Let <span class="math inline">\(L_{\Lda}^F\)</span> be the misclassification rate of the above classifier for distribution <span class="math inline">\(F\)</span>. Assuming equal class prior and centered means, <span class="math inline">\(\pi_0=\pi_1\)</span> and <span class="math inline">\((\bmu_0+\bmu1)/2=\mb{0}\)</span>, re-arranging a bit, we obtain</p>
<p><span class="math display">\[
\begin{aligned}
g_{\Lda}(\bx) :=  \argmin_y \bx\T \bSig^{-1} \bmu_y.\end{aligned}
\]</span></p>
<p>In words, the  classifier chooses the class for whom the projection of an input vector <span class="math inline">\(\bx\)</span>, onto <span class="math inline">\(\bSig^{-1} \bmu_y\)</span>, is maximized. When there are only two classes, this further simplies to</p>
<p><span class="math display">\[
\begin{aligned}
g_{2-\Lda}(\bx) :=  \II\{ \bx\T \bSig^{-1} \bdel &gt; 0 \},\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\bdel=\bmu_0-\bmu_1\)</span>. Note that the equal class prior and centered means assumptions merely changes the threshold constant from <span class="math inline">\(0\)</span> to something else.</p>
<h2> Model</h2>
<p>A statistical model is a family of distributions indexed by a parameter <span class="math inline">\(\bth \in \bTh\)</span>, <span class="math inline">\(\mc{F}_{\bth}=\{F_{\bth} : \bth \in \bTh \}\)</span>. Consider the special case of the above where <span class="math inline">\(F_{\bX|Y=y}\)</span> is a multivariate Gaussian distribution, <span class="math inline">\(\mc{N}(\bmu_y,\bSig)\)</span>, where each class has its own mean, but all classes have the same covariance. We refer to this model as the  model. Let <span class="math inline">\(\bth=(\bpi,\bmu,\bSig)\)</span>, and let <span class="math inline">\(\bTh_{C-\Lda}=( \triangle_C, \Real^{p \times C},\Real_{\succ 0}^{p \times p})\)</span>, where <span class="math inline">\(\bmu=(\bmu_1,\ldots, \bmu_C)\)</span>, <span class="math inline">\(\triangle_C\)</span> is the <span class="math inline">\(C\)</span> dimensional simplex, that is <span class="math inline">\(\triangle_C = \{ \bx : x_i \geq 0 \forall i, \sum_i x_i = 1\}\)</span>, and <span class="math inline">\(\Real_{\succ 0}^{p \times p}\)</span> is the set of positive definite <span class="math inline">\(p \times p\)</span> matrices. Denote <span class="math inline">\(\mc{F}_{\Lda}=\{F_{\bth} : \bth \in \bTh_{\Lda}\}\)</span>, dropping the superscript <span class="math inline">\(C\)</span> for brevity where appropriate. The following lemma is well known:</p>
<p><span class="math inline">\(L_{\Lda}^F=L_*^F\)</span> for any <span class="math inline">\(F \in \mc{F}_{\Lda}\)</span>.</p>
<p>Under the  model, the Bayes optimal classifier is available by plugging the explicit distributions into Eq. .</p>
<h1>Theory When <span class="math inline">\(\bth\)</span> is Given</h1>
<h2> is rotationally invariant</h2>
<p>For certain classification tasks, the ambient coordinates have intrinsic value, for example, when simple interpretability is desired. However, in many other contexts, interpretability is less important <span class="citation" data-cites="Breiman2001b">(<span class="citeproc-not-found" data-reference-id="Breiman2001b"><strong>???</strong></span>)</span>. When the exploitation task at hand is invariant to rotations, then we have no reason to restrict our search space to be sparse in the ambient coordinates, rather, for example, we can consider sparsity in the eigenvector basis. Fisherfaces is one example of a rotationally invariant classifier, under certain model assumptions. Let <span class="math inline">\(\bW\)</span> be a rotation matrix, that is <span class="math inline">\(\bW \in \mc{W}=\{\bW : \bW\T = \bW^{-1}\)</span> and det<span class="math inline">\((\bW)=1\}\)</span>. Moreover, let <span class="math inline">\(\bW \circ F\)</span> denote the distribution <span class="math inline">\(F\)</span> after transformation by an operator <span class="math inline">\(\bW\)</span>. For example, if <span class="math inline">\(F=\mc{N}(\bmu,\bSig)\)</span> then <span class="math inline">\(\bW \circ F=\mc{N}(\bW \bmu, \bW \bSig \bW\T)\)</span>.</p>
<p>A rotationally invariant classifier has the following property:</p>
<p><span class="math display">\[
L_g^F = L_g^{W \circ F}, \qquad F \in \mc{F}.
\]</span></p>
<p>In words, the Bayes risk of using classifier <span class="math inline">\(g\)</span> on distribution <span class="math inline">\(F\)</span> is unchanged if <span class="math inline">\(F\)</span> is first rotated, for any <span class="math inline">\(F \in \mc{F}\)</span>.</p>
<p>Now, we can state the main lemma of this subsection:  is rotationally invariant.</p>
<p>[lem:rot] <span class="math inline">\(L_{\Lda}^F = L_{\Lda}^{W \circ F}\)</span>, for any <span class="math inline">\(F \in \mc{F}\)</span>.</p>
<p> simply becomes thresholding <span class="math inline">\(\bx\T \bSig^{-1} \bdel\)</span>. Thus, we can demonstrate rotational invariance by demonstrating that <span class="math inline">\(\bx\T \bSig^{-1} \bdel\)</span> is rotationally invariant.</p>
<p><span class="math display">\[
\begin{aligned}
% \bx\T \bSig^{-1} \bdel &amp;= 
(\bW \bx) \T  (\bW \bSig \bW\T )^{-1} \bW \bdel  %&amp; \text{from Lemma \ref{lem:rot}}\\
&amp;= \bx\T \bW\T  (\bW \bU \bS \bU\T \bW\T)^{-1} \bW \bdel &amp; \text{by substituting $\bU \bS \bU\T$ for $\bSig$} \\
&amp;= \bx\T \bW\T  (\mt{\bU} \bS \mt{\bU}\T)^{-1} \bW \bdel &amp; \text{by letting $\mt{\bU}=\bW \bU$} \\
&amp;= \bx\T \bW\T  (\mt{\bU} \bS^{-1} \mt{\bU}\T) \bW \bdel &amp; \text{by the laws of matrix inverse} \\
&amp;= \bx\T \bW\T  \bW \bU \bS^{-1}  \bU\T \bW\T \bW \bdel &amp; \text{by un-substituting $\bW \bU=\mt{\bU}$} \\
&amp;= \bx\T  \bU \bS^{-1}  \bU\T  \bdel  &amp; \text{because $\bW\T \bW = \bI$} \\
&amp;= \bx\T   \bSig^{-1} \bdel &amp; \text{by un-substituting $\bU \bS^{-1} \bU\T = \bSig$}\end{aligned}
\]</span></p>
<p>One implication of this lemma is that we can reparameterize without loss of generality. Specifically, defining <span class="math inline">\(\bW := \bU\T\)</span> yields a change of variables: <span class="math inline">\(\bSig \mapsto \bS\)</span> and <span class="math inline">\(\bdel \mapsto \bU\T \bdel := \mt{\bdel}\)</span>, where <span class="math inline">\(\bS\)</span> is a diagonal covariance matrix. Moreover, let <span class="math inline">\(\bd=(\sigma_1,\ldots, \sigma_D)\T\)</span> be the vector of eignevalues, then <span class="math inline">\(\bS^{-1} \mt{\bdel}=\bd^{-1} \odot \mt{\bdel}\)</span>, where <span class="math inline">\(\odot\)</span> is the Hadamard (entrywise) product. The  classifier may therefore be encoded by a unit vector, <span class="math inline">\(\mt{\bd}:= \frac{1}{m} \bd^{-1} \odot \mt{\bdel}\)</span>, and its magnitude, <span class="math inline">\(m:=\norm{\bd^{-1} \odot \mt{\bdel}}\)</span>.</p>
<h2>Projection Based Classifiers</h2>
<p>Let <span class="math inline">\(\bA \in \Real^{d \times p}\)</span> be an orthonormal matrix, that is, a matrix that projects p dimensional data into a d dimensional subspace, where <span class="math inline">\(\bA\bA\T\)</span> is the <span class="math inline">\(d \times d\)</span> identity matrix, and <span class="math inline">\(\bA\T \bA\)</span> is symmetric <span class="math inline">\(p \times p\)</span> matrix with rank d. The question that motivated this work is: what is the best projection matrix that we can estimate, to use to “pre-process” the data prior to applying . Projecting the data <span class="math inline">\(\bx\)</span> onto a low-dimensional subspace, and the classifying via  in that subspace is equivalent to redefining the parameters in the low-dimensional subspace, <span class="math inline">\(\bSig_A=\bA \bSig \bA\T \in \Real^{d \times d}\)</span> and <span class="math inline">\(\bdel_A = \bA \bdel \in \Real^d\)</span>, and then using <span class="math inline">\(g_{\Lda}\)</span>. When <span class="math inline">\(C=2\)</span>, <span class="math inline">\(\pi_0=\pi_1\)</span>, and <span class="math inline">\((\mu_0+\mu_1)/2=\mb{0}\)</span>, this amounts to:</p>
<p><span class="math display">\[
\begin{aligned}
 \label{eq:g_A}
g^d_A(x) := \II \{ (\bA \bx)\T \bSig^{-1}_A \bdel_A &gt; 0\}, \text{ where } \bA \in \Real^{d \times p}.\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(L^d_A :=\int \PP[g_A(\bx)=y] f_{\bx,y} d\bx dy\)</span>. Our goal therefore is to be able to choose <span class="math inline">\(A\)</span> for a given parameter setting <span class="math inline">\(\bth=(\bpi, \bdel,\bSig)\)</span>, such that <span class="math inline">\(L_A\)</span> is as small as possible (note that <span class="math inline">\(L_A\)</span> will never be smaller than <span class="math inline">\(L_*\)</span>).</p>
<p>Formally, we seek to solve the following optimization problem:</p>
<p><span class="math display">\[
\label{eq:A}
\begin{aligned}
&amp; \underset{\bA}{\text{minimize}}
&amp; &amp; \EE [ \II \{ \bx\T \bA\T \bSig^{-1}_A \bdel_A &gt; 0\} \neq y] \\
&amp; \text{subject to} &amp; &amp; \bA \in \Real^{p \times d}, \quad \bA \bA\T = \bI_{d \times d},
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\bI_{u \times v}\)</span> is the <span class="math inline">\(u \times v\)</span> identity matrix identity, that is, <span class="math inline">\(\bI(i,j)=1\)</span> for all <span class="math inline">\(i=j \leq \min(u,v)\)</span>, and zero otherwise. Let <span class="math inline">\(\mc{A}^d=\{\bA : \bA \in \Real^{d \times p}, \bA \bA\T = \bI_{d \times d}\}\)</span>, and let <span class="math inline">\(\mc{A}_* \subset \mc{A}\)</span> be the set of <span class="math inline">\(\bA\)</span> that minimize Eq. , and let <span class="math inline">\(\bA_* \in \mc{A}_*\)</span> (where we dropped the superscript <span class="math inline">\(d\)</span> for brevity). Let <span class="math inline">\(L_{\bA}^*=L_{\bA_*}\)</span> be the misclassification rate for any <span class="math inline">\(\bA \in \mc{A}_*\)</span>, that is, <span class="math inline">\(L_{\bA}^*\)</span> is the Bayes optimal misclassification rate for the classifier that composes <span class="math inline">\(\bA\)</span> with .</p>
<p>In our opinion, Eq.  is the simplest supervised manifold learning problem there is: a two-class classification problem, where the data are multivariate Gaussians with shared covariances, the manifold is linear, and the classification is done via . Nonetheless, solving Eq.  is difficult, because we do not know how to evaluate the integral analytically, and we do not know any algorithms that are guaranteed to find the global optimum in finite time. This has led to previous work using a surrogate function <span class="citation" data-cites="not sure who">(<span class="citeproc-not-found" data-reference-id="not"><strong>???</strong></span>; <span class="citeproc-not-found" data-reference-id="sure"><strong>???</strong></span>; <span class="citeproc-not-found" data-reference-id="who"><strong>???</strong></span>)</span>. We proceed by studying a few natural choices for <span class="math inline">\(\bA\)</span>.</p>
<h3>Bayes Optimal Projection</h3>
<p><span class="math inline">\(\bdel\T \bSig^{-1} \in \mc{A}_*\)</span></p>
<p>Let <span class="math inline">\(\bB = (\bSig^{-1} \bdel)\T = \bdel\T (\bSig^{-1})\T = \bdel\T \bSig^{-1}\)</span>, so that <span class="math inline">\(\bB\T = \bSig^{-1} \bdel\)</span>, and plugging this in to Eq. , we obtain</p>
<p><span class="math display">\[
\begin{aligned}
g_{B}(x) &amp;= \II \{ \bx \bB\T  \bSig^{-1}_{B} \bdel_{B} &gt; 0\} &amp;
\\&amp;= \II \{ \bx\T \bSig^{-1} \bdel \times (\bSig^{-1}_{B} \bdel_{B}) &gt; 0\} &amp; \text{plugging in $\bB$}
\\&amp;= \II \{ \bx\T \bSig^{-1} \bdel k &gt; 0\} &amp; \text{because $\bSig^{-1}_{B} \bdel_{B} &gt; 0$}.\end{aligned}
\]</span></p>
<p>In other words, letting <span class="math inline">\(\bB\)</span> be the Bayes optimal projection recovers the Bayes classifier, as it should. Or, more formally, for any <span class="math inline">\(F \in \mc{F}_{\Lda}\)</span>, <span class="math inline">\(L_{\bdel\T \bSig^{-1}} = L_*\)</span></p>
<h3>Principle Components Analysis () Projection</h3>
<p>Principle Components Analysis () finds the directions of maximal variance in a dataset.  is closely related to eigendecompositions and singular value decompositions (). In particular, the top principle component of a matrix <span class="math inline">\(\bX \in \Real^{p \times n}\)</span>, whose columns are centered, is the eigenvector with the largest corresponding eigenvalue of the centered covariance matrix <span class="math inline">\(\bX \bX\T\)</span>.  enables one to estimate this eigenvector without ever forming the outer product matrix, because  factorizes a matrix <span class="math inline">\(\bX\)</span> into <span class="math inline">\(\bU \bS \bV\T\)</span>, where <span class="math inline">\(\bU\)</span> and <span class="math inline">\(\bV\)</span> are orthonormal <span class="math inline">\({p \times n}\)</span> matrices, and <span class="math inline">\(\bS\)</span> is a diagonal matrix, whose diagonal values are decreasing, <span class="math inline">\(s_1 \geq s_2 \geq \cdots &gt; s_n\)</span>. Defining <span class="math inline">\(\bU =[\bu_1, \bu_2, \ldots, \bu_n]\)</span>, where each <span class="math inline">\(\bu_i \in \Real^p\)</span>, then <span class="math inline">\(\bu_i\)</span> is the <span class="math inline">\(i^{th}\)</span> eigenvector, and <span class="math inline">\(s_i\)</span> is the square root of the <span class="math inline">\(i^{th}\)</span> eigenvalue of <span class="math inline">\(\bX \bX\T\)</span>. Let <span class="math inline">\(\bA^{\Pca}_d =[\bu_1, \ldots , \bu_d]\)</span> be the truncated  orthonormal matrix.</p>
<p>The  matrix is perhaps the most obvious choice of a orthonormal matrix for several reasons. First, truncated  minimizes the squared error loss between the original data matrix and all possible rank d representations:</p>
<p><span class="math display">\[
\begin{aligned}
\argmin_{A \in \Real^{d \times p} : \bA \bA\T = \bI_{d \times d}} \norm{ \bX - \bA^T \bA }_F^2.\end{aligned}
\]</span></p>
<p>Second, the ubiquity of  has led to a large number of highly optimized numerical libraries for computing  (for example, LAPACK <span class="citation" data-cites="Anderson1999a">Anderson et al. (<a href="#ref-Anderson1999a">1999</a>)</span>).</p>
<p>Moreover, let <span class="math inline">\(\bU_d=[\bu_1,\ldots,\bu_d] \in \Real^{p \times d}\)</span>, and note that <span class="math inline">\(\bU_d\T \bU_d = \bI_{d \times p}\)</span> and <span class="math inline">\(\bU_d\T \bU_d = \bI_{p \times d}\)</span>. Similarly, let <span class="math inline">\(\bU \bS \bU\T = \bSig\)</span>, and <span class="math inline">\(\bU \bS^{-1} \bU\T = \bSig^{-1}\)</span>. Let <span class="math inline">\(\bS_d\)</span> be the matrix whose diagonal entries are the eigenvalues, up to the <span class="math inline">\(d^{th}\)</span> one, that is <span class="math inline">\(\bS_d(i,j)=s_i\)</span> for <span class="math inline">\(i=j \leq d\)</span> and zero otherwise. Similarly, <span class="math inline">\(\bSig_d=\bU \bS_d \bU\T=\bU_d \bS_d \bU_d\T\)</span>.</p>
<p>Let <span class="math inline">\(g_{\Pca}^d:=g_{A_{\Pca}^d}\)</span>, and let <span class="math inline">\(L_{\Pca}^d:=L_{A_{\Pca}^d}\)</span>. And let <span class="math inline">\(g_{\Lda}^d := \II \{ x \bSig_d^{-1} \bdel &gt; 0\}\)</span> be the regularized  classifier, that is, the  classifier, but sets the bottom <span class="math inline">\(p-d\)</span> eigenvalues to zero.</p>
<p><span class="math inline">\(L_{A_{\Pca}}^d = L_{\Lda}^d\)</span>.</p>
<p>Plugging <span class="math inline">\(\bU_d\)</span> into Eq.  for <span class="math inline">\(\bA\)</span>, and considering only the left side of the operand, we have</p>
<p><span class="math display">\[
\begin{aligned}
(\bA \bx)\T \bSig^{-1}_A \bdel_A &amp;= \bx\T \bA\T \bA \bSig^{-1} \bA\T \bA \bdel,
\\&amp;= \bx\T  \bU_d\bU_d\T \bSig^{-1} \bU_d\bU_d\T \bdel,
\\&amp;= \bx\T  \bU_d \bU_d\T \bU \bS^{-1} \bU \bU_d\bU_d\T \bdel,
\\&amp;= \bx\T  \bU_d \bI_{d \times p} \bS^{-1} \bI_{p \times d} \bU_d\T \bdel,
\\&amp;= \bx\T  \bU_d \bS^{-1}_d  \bU_d\T \bdel ,
\\&amp;= \bx\T  \bSig^{-1}_d  \bdel.\end{aligned}
\]</span></p>
<p>The implication of this lemma is that if one desires to implement Fisherfaces, rather than first learning the eigenvectors and then learning , one can instead directly implement regularized  by setting the bottom <span class="math inline">\(p-d\)</span> eigenvalues to zero.</p>
<h3>Linear Optimal Low-Rank () Projection</h3>
<p>The basic idea of  is to use both <span class="math inline">\(\bdel\)</span> and the top <span class="math inline">\(d\)</span> eigenvectors. Most naïvely, we could simply concatenate the two, <span class="math inline">\(\bA_{\Lol}^d=[\bdel,\bA_{\Pca}^{d-1}]\)</span>. Recall that eigenvectors are orthonormal. To maintain orthonormality, we could easily apply Gram-Schmidt, <span class="math inline">\(\bA_{\Lol}^d= ([\bdel, \bA_{\Pca}^{d-1}])\)</span>. Both in practice and in theory (as will be shown below), this orthogonalization step does not matter much.</p>
<p>to ensure that they are balanced appropriately, we normalize <span class="math inline">\(\bdel\)</span></p>
<p>each vector in <span class="math inline">\(\bdel\)</span> to have norm unity. Formally, let <span class="math inline">\(\mt{\bdel}_j = \bdel_j / \norm{\bdel_j}\)</span>, where <span class="math inline">\(\bdel_j\)</span> is the <span class="math inline">\(j^{th}\)</span> difference of the mean vector (remember, the number of vectors is equal to <span class="math inline">\(C-1\)</span>, where <span class="math inline">\(C\)</span> is the total number of classes), and let <span class="math inline">\(\bA_{\Lol}^d=[\mt{\bdel}, \bA_{\Pca}^{d-(C-1)}]\)</span>. The eigenvectors are all normalized and orthogonal to one another; to impose orthogonality between <span class="math inline">\(\mt{\bdel}\)</span> and the eigenvectors, we could use any number of numerically optimized algorithms. However, in practice, orthogonalizing does not matter very much, so we do not bother. We formally demonstrate this below.</p>
<h3>Rotation of Projection Based Linear Classifiers <span class="math inline">\(g_A\)</span></h3>
<p>By a similar arguement as above, one can easily show that:</p>
<p><span class="math display">\[
\begin{aligned}
(\bA  \bW \bx) \T  (\bA \bW  \bSig  \bW\T \bA\T)^{-1} \bA \bW \bdel 
&amp;= \bx\T (\bW\T \bA\T) (\bA \bW) \bSig^{-1} (\bW\T \bA\T) (\bA \bW) \bdel \\
&amp;= \bx\T \bY\T \bY \bSig^{-1} \bY\T \bY \bdel \\
&amp;= \bx\T \bZ \bSig^{-1} \bZ\T \bdel \\
&amp;= \bx\T (\bZ \bSig \bZ\T)^{-1} \bdel = \bx\T \mt{\bSig}_d^{-1} \bdel,
% (\bA\T \bA \bx) \T  \bSig^{-1} \bA\T \bA \bdel = (\bA \bx)\T \bSig^{-1}_A \bdel_A.\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\bY = \bA \bW \in \Real^{d \times p}\)</span> so that <span class="math inline">\(\bZ=\bY\T \bY\)</span> is a symmetric <span class="math inline">\({p \times p}\)</span> matrix of rank <span class="math inline">\(d\)</span>. In other words, rotating and then projecting is equivalent to a change of basis. The implications of the above is:</p>
<p><span class="math inline">\(g_A\)</span> is rotationally invariant if and only if span(<span class="math inline">\(\bA\)</span>)=span(<span class="math inline">\(\bSig_d\)</span>). In other words,  is the only rotationally invariant projection.</p>
<h2> versus</h2>
<p>We would like to prove that  is always better than , when using one or the other to project the data onto a low dimensional space, followed by classifying with . Formally, we would like to prove this:</p>
<p>[thm:L&gt;P] <span class="math inline">\(\PP[L_{\Lol}^d \leq L_{\Pca}^d]=1\)</span> for any <span class="math inline">\(\bth \in \bTh_{\Lda}\)</span> and <span class="math inline">\(d &lt; p\)</span>.</p>
<p>This means that for any distribution in the two-class linear discriminant analysis model,  yields a projection that is better than  , given that either will be composed with ; and better is defined with respect to classifier performance. And that this result is true regardless of the dimensionality of the projection. This is the formal statement of Theorem [thm:LDA].</p>
<h3>Proof of Theorem [thm:L&gt;P]</h3>
<p>To build up to being able to prove Theorem [thm:L&gt;P], we prove a sequence of increasingly more sophisticated statements under the two-class model, <span class="math inline">\(\bTh_{2-\Lda}\)</span>:</p>
<p>when <span class="math inline">\(\bSig\)</span> is a scaled identity matrix, and when we set <span class="math inline">\(d=1\)</span>,  is optimal, whereas  has no such guarantee;</p>
<p>when <span class="math inline">\(\bSig\)</span> is a diagonal matrix, and we set <span class="math inline">\(d=1\)</span>,  is better than  with high probability;</p>
<p>when <span class="math inline">\(\bSig\)</span> is a scaled identity matrix, and we set <span class="math inline">\(d&lt;p\)</span> , is better than  with high probability;</p>
<p>when <span class="math inline">\(\bSig\)</span> is a diagonal matrix, and we set <span class="math inline">\(d&lt;p\)</span> , is better than  with high probability;</p>
<p>when <span class="math inline">\(\bSig\)</span> is an arbitrary matrix, and we set <span class="math inline">\(d=1\)</span> , is better than  with high probability;</p>
<p>when <span class="math inline">\(\bSig\)</span> is an arbitrary matrix, and we set <span class="math inline">\(d&lt;p\)</span> , is better than  with high probability;</p>
<p>Extensions to the multiclass setting might then also be explored.</p>
<p>Recalling Eq. , a projection based classifier is effectively thresholding the dot product of <span class="math inline">\(\bx\)</span> with the linear projection operator <span class="math inline">\(\bP_A :=\bA\T \bSig_A^{-1} \bdel_A \in \Real^p\)</span>, and let <span class="math inline">\(\bP_*:=\bP_{\bA_*}\)</span>. Unfortunately, the nonlinearity in in Eq.  makes analysis difficult. However, because of the linear nature of the classifier and projection matrix operator, an objective function that is simpler to evaluate is available. Define <span class="math inline">\(\angle(\bP,\bP&#39;) = \frac{ \bP\T \bP&#39;}{||\bP||_2 ||\bP&#39;||_2} \in (0,1)\)</span>, and consider</p>
<p><span class="math display">\[
\label{eq:angle}
\begin{aligned}
&amp; \underset{\bA}{\text{minimize}}
&amp; &amp; -\angle(\bP_A,\bP_*), 
\\ &amp; \text{subject to} &amp; &amp; \bA \in \Real^{p \times d}, \quad \bA \bA\T = \bI_{d \times d}.
\end{aligned}
\]</span></p>
<p>[lem:angle] The solution to Eq.  is also the solution to Eq.  for any given <span class="math inline">\(d\)</span>.</p>
<p>The minimum of Eq.  is clearly <span class="math inline">\(\bA=\bSig^{-1} \bdel\)</span>, which is also the minimum of Eq. .</p>
<p><span class="math inline">\(\angle\)</span> is merely the angle between two vectors, and is therefore scale invariant. In other words, <span class="math inline">\(\angle(\bP_A,\bP)=\angle(\bP_{c A},\bP)\)</span>, for any <span class="math inline">\(c &gt; 0\)</span>.</p>
<p>Given the above, we can evaluate various choices of <span class="math inline">\(\bA\)</span> in terms of their induced projection operator <span class="math inline">\(\bP_A\)</span> and the angle between said projection operators and the Bayes optimal projection operator.</p>
<p>A corollary to the above is:</p>
<p>[cor:angle] <span class="math inline">\(\angle(\bP_A,\bP_*) \leq \angle(\bP_{A&#39;},\bP_*) \implies L_A \leq L_{A&#39;}\)</span>.</p>
<p>i’m not sure this is true.</p>
<p>Note that Corollary [cor:angle] is a stronger statement than Lemma [lem:angle], and in particular, Corollary [cor:angle] implies Lemma [lem:angle].</p>
<h4>Scaled Identity Covariance Matrix and <span class="math inline">\(d=1\)</span></h4>
<p><span class="math inline">\(\bA_{\Lol}^1 \in \mc{A}_*\)</span> when <span class="math inline">\(\bSig=\bI\)</span> and <span class="math inline">\(\pi_0=\pi_1\)</span> and <span class="math inline">\((\mu_0+\mu_1)/2=\mb{0}\)</span>.</p>
<p>When <span class="math inline">\(\bSig=c \bI\)</span>, that <span class="math inline">\(\bA_* = \bSig^{-1} \bdel = c \bdel\)</span>. By definition, the first projection vector of <span class="math inline">\(\bA_{\Lol}\)</span> is <span class="math inline">\(\bdel\)</span>. And by Lemma [lem:angle], <span class="math inline">\(\angle(\bP_A,\bP_*)=\angle(\bP_{c A},\bP_*)\)</span>, for any <span class="math inline">\(c&gt;0\)</span>, and in this scenario, <span class="math inline">\(\bA_{\Lol}^1=c \bA_*\)</span>.</p>
<h4>Diagonal covariance matrix and <span class="math inline">\(d=1\)</span></h4>
<p>Now consider <span class="math inline">\(\bSig=\bS\)</span> is diagonal, <span class="math inline">\(\pi_0=\pi_1\)</span> and <span class="math inline">\((\mu_0+\mu_1)/2=\mb{0}\)</span>, and <span class="math inline">\(\bdel \in \Real^p\)</span>. We want to know how often is  better than .</p>
<p>Let <span class="math inline">\(\bs\)</span> be the singular values of <span class="math inline">\(\bSig\)</span>. When <span class="math inline">\(\bSig\)</span> is diagonal, its diagonal elements are <span class="math inline">\(s_1,\ldots,s_p\)</span>, and we call that matrix <span class="math inline">\(\bS\)</span>. Recalling that <span class="math inline">\(s_i=\lambda_i^2\)</span>, where <span class="math inline">\(\lambda_i^2\)</span> are eigenvalues, and when a matrix is diagonal, <span class="math inline">\(\lambda_i=\sigma_i\)</span>, we have <span class="math inline">\(\bA_*=\bS^{-1} \bdel=\bs^{-1} \odot \bdel:=(\del_1/\sigma_1^2,\ldots,\del_p/\sigma_p^2)\)</span>. By definition, <span class="math inline">\(\bA_{\Lol}^1=\bdel\)</span>, and <span class="math inline">\(\bA_{\Pca}^1=\bu_1\)</span>, where <span class="math inline">\(\bu_1\)</span> is the eigenvector associated with the largest eigenvalue of <span class="math inline">\(\bS\)</span>.</p>
<p>We consider a simple version of our question: how often is the angle between <span class="math inline">\(\bA_{\Lol}^1\)</span> and <span class="math inline">\(\bA_*\)</span> bigger than the angle between <span class="math inline">\(\bA_{\Pca}^1\)</span> and <span class="math inline">\(\bA_*\)</span>. Formally, we would like to prove:</p>
<p><span class="math inline">\(\PP[ \angle(A_{\Lol}^1,\bA_*) \geq \angle(\bA_{\Pca}^1,\bA_*)]=1\)</span> for any <span class="math inline">\(\bth \in \bTh_{2-\Lda}\)</span>.</p>
<p>coming soon. numerical experiments are convincing.</p>
<p>Given that, we seek to prove</p>
<p><span class="math inline">\(\PP[L_{\Lol}^1 &lt; L_{\Pca}^1]=1\)</span> when <span class="math inline">\(\bs \sim F_s\)</span>, <span class="math inline">\(\bdel \sim F_{\delta}\)</span> and <span class="math inline">\(\bmu_1=\bmu_0\)</span> and <span class="math inline">\(\pi_0=\pi_1\)</span>.</p>
<p>coming soon, numerical experiments are convincing.</p>
<h4>Scaled Identity covariance matrix and <span class="math inline">\(d&gt;1\)</span></h4>
<h4>Diagonal covariance matrix and <span class="math inline">\(d&gt;1\)</span></h4>
<p>Things we would like to prove include</p>
<p><span class="math inline">\(\PP[L_{\Lol}^d \leq L_{\Pca}^d]=1\)</span> when <span class="math inline">\(\bSig=c \bI\)</span>, <span class="math inline">\(\pi_0=\pi_1\)</span> and <span class="math inline">\((\mu_0+\mu_1)/2=\mb{0}\)</span>, for any <span class="math inline">\(d &lt; p\)</span>.</p>
<p><span class="math inline">\(\PP[L_{\Lol}^d \leq L_{\Pca}^d]=1\)</span> when <span class="math inline">\(\bSig=\bS\)</span>, <span class="math inline">\(\pi_0=\pi_1\)</span> and <span class="math inline">\((\mu_0+\mu_1)/2=\mb{0}\)</span>, for any <span class="math inline">\(d &lt; p\)</span>.</p>
<h4>Arbitrary covariance matrix and <span class="math inline">\(d=1\)</span></h4>
<h4>Arbitrary covariance matrix and <span class="math inline">\(d&gt;1\)</span></h4>
<h1>Asymptotic Theory</h1>
<p>In real data problems, the true joint distribution is unknown. Instead, what is provided is a set of training data. We therefore assume the existence of <span class="math inline">\(n\)</span> training samples, each of which has been sampled identically and independently from the same distribution, <span class="math inline">\((\bX_i,Y_i) \iid F_{\bX,Y}\)</span>, for <span class="math inline">\(i =1,2,\ldots, n\)</span>. We can use these training samples to then estimate <span class="math inline">\(f_{x|y}\)</span> and <span class="math inline">\(f_y\)</span>. Plugging these estimates in to Eq. , we obtain the Bayes plugin classifier:</p>
<p><span class="math display">\[
\begin{aligned}
 \label{eq:plugin}
\mh{g}^*_n(\bx) := \argmax_y \mh{f}_{\bx|y}\mh{f}_y.\end{aligned}
\]</span></p>
<p>Under suitable conditions, it is easy to show that this Bayes plugin classifiers performance is asymptotically optimal. Formally, we know that: <span class="math inline">\(L_{\mh{g}^*_n} \conv L_{g^*}\)</span>.</p>
<p>When the parameters, and we want to use a linear approach, we can implement a Bayes plug-in , which we call Fisher’s Discriminant Analysis () <span class="citation" data-cites="Fisher1925a">Fisher (<a href="#ref-Fisher1925a">1925</a>)</span>. Under the two-class, equal prior, and centered means assumption, we have</p>
<p><span class="math display">\[
\begin{aligned}
\mh{g}^*_n(\bx) := \II\{ \bx\T \mh{\bSig}^{-1} \mh{\bdel} &gt; 0 \},\end{aligned}
\]</span></p>
<p>and <span class="math inline">\(L_{\mh{g}_n}\)</span> is the misclassification rate for an estimated classifier, <span class="math inline">\(\mh{g}_n\)</span>. Unfortunately, when <span class="math inline">\(p \gg n\)</span>, the estimate of the covariance matrix <span class="math inline">\(\bSig\)</span> will be low-rank, and therefore, not invertible (because an infinite number of solutions all fit equally well). In such scenarios, we seek alternative methods, even in the  model.</p>
<p>We would like to prove:</p>
<p><span class="math inline">\(L_{\mh{\Lol}}^d \conv L_*\)</span> for any <span class="math inline">\(\bth \in \bTh_{2-\Lda}\)</span>.</p>
<p><span class="math inline">\(\PP[L_{\mh{\Pca}}^d \conv L_*]&gt;0\)</span> for any <span class="math inline">\(\bth \in \bTh_{2-\Lda}\)</span></p>
<h1>Finite Sample Theory</h1>
<p>It would be awesome to prove something like:</p>
<p><span class="math display">\[
\begin{aligned}
\PP[ \mh{L}_{\Lol}^d - \mh{L}_{\Pca}^2  &gt; 0  ] &lt; f(\bth,d,n),\end{aligned}
\]</span></p>
<p>which would state that  is better than , again, under suitable assumptions.</p>
<p><span class="math display">\[
\begin{aligned}
\PP[ \bP_{\Pca} \T \bP_*  - \bP_{\Lol} \T \bP_*  &gt; t \norm{\bP_A} \norm{\bP_*} ] &lt; f(t,p,d),\end{aligned}
\]</span></p>
<p>which would state that  is better than , again, under suitable assumptions.</p>
<p>In terms of distributiosn of the above, it seems that perhaps we could start simple. Assume for the moment that <span class="math inline">\(\bdel,\bu_1,\ldots,\bu_p \iid \mc{N}(\bmu_p, \bSig_p)\)</span>, and let <span class="math inline">\(\bLam=(\bu_1,\ldots,\bu_p)\T\)</span>, and <span class="math inline">\(\bSig = \bLam\T \bLam\)</span>.</p>
<p>The reason the above is probabilistic is because it is under certain assumptiosn on the <em>distributions</em> of <span class="math inline">\(bdel\)</span>, <span class="math inline">\(\bSig\)</span>, and <span class="math inline">\(\bA\)</span>.</p>
<p>Perhaps even simpler is to start with specific assumptions about <span class="math inline">\(\bdel\)</span>, <span class="math inline">\(\bSig\)</span>, and <span class="math inline">\(\bA\)</span>. Because  is rotationally invariant, I believe that we can assert, without loss of generality, that <span class="math inline">\(\bSig=\bS\)</span>, where <span class="math inline">\(\bS\)</span> is a diagonal matrix with diagonal entries <span class="math inline">\(\sigma_1,\ldots, \sigma_p\)</span>, where all <span class="math inline">\(\sigma_j &gt; 0\)</span>. Now, the optimal projection <span class="math inline">\(\bSig^{-1} \bdel\)</span> is just a simple dot product, <span class="math inline">\(\bd\T \bdel\)</span>, where <span class="math inline">\(\bd=\)</span>diag(<span class="math inline">\(\bS\)</span>)<span class="math inline">\(\in \Real^p\)</span>.</p>
<p>For example, letting <span class="math inline">\(\bA=\bU_d\)</span>, and letting <span class="math inline">\(\bU_i=e_i\)</span> be the unit vector, with zeros everywhere except a one in the <span class="math inline">\(i^{th}\)</span> position, we have</p>
<p><span class="math display">\[
\begin{aligned}
\bP_A\T \bP_* %&amp;
= \bdel\T \bU_d\T \bU_d \bSig^{-1} \bU_d\T \bU_d \bSig^{-1} \bdel %\\ = 
\bdel\T \bSig_d \bSig^{-1} \bSig_d \bSig^{-1} \bdel %\\&amp;
= \bdel\T \bSig^{-2} \bdel.\end{aligned}
\]</span></p>
<p>So, we want to understand the probability that <span class="math inline">\(\alpha_{\Pca}\)</span> is small under different parameter settings, <span class="math inline">\(\bth \in \bTh\)</span>.</p>
<div class="references">
<div id="ref-Allard2012">
<p>Allard, William K., Guangliang Chen, and Mauro Maggioni. 2012. “Multi-scale geometric methods for data sets II: Geometric Multi-Resolution Analysis.” <em>Applied and Computational Harmonic Analysis</em> 32 (3). Elsevier Inc.: 435–62. <a href="http://doi.org/10.1016/j.acha.2011.08.001">doi:10.1016/j.acha.2011.08.001</a>.</p>
</div>
<div id="ref-Anderson1999a">
<p>Anderson, E., Z. Bai, C. Bischof, S. Blackford, J. Demmel, J. Dongarra, J. Du Croz, et al. 1999. <em>LAPACK Users’ Guide: Third Edition</em>. SIAM. <a href="https://books.google.com/books?hl=en\&amp;lr=\&amp;id=AZlvEnr9gCgC\&amp;pgis=1" class="uri">https://books.google.com/books?hl=en\&amp;lr=\&amp;id=AZlvEnr9gCgC\&amp;pgis=1</a>.</p>
</div>
<div id="ref-Baik2006a">
<p>Baik, Jinho, and Jack W. Silverstein. 2006. “Eigenvalues of large sample covariance matrices of spiked population models.” <em>Journal of Multivariate Analysis</em> 97 (6): 1382–1408.</p>
</div>
<div id="ref-Belhumeur1997a">
<p>Belhumeur, Peter N., Joäo P. Hespanha, and David J. Kriegman. 1997. “Eigenfaces vs. fisherfaces: Recognition using class specific linear projection.” <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 19 (7): 711–20.</p>
</div>
<div id="ref-Belkin2003a">
<p>Belkin, Mikhail, and Partha Niyogi. 2003. “Laplacian Eigenmaps for Dimensionality Reduction and Data.” <em>Neural Computation</em> 15: 1373–96.</p>
</div>
<div id="ref-Belkin2006a">
<p>Belkin, Mikhail, Partha Niyogi, and Vikas Sindhwani. 2006. “Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples.” <em>The Journal of Machine Learning Research</em> 7 (December). JMLR.org: 2399–2434. <a href="http://doi.org/10.1016/j.neuropsychologia.2009.02.028">doi:10.1016/j.neuropsychologia.2009.02.028</a>.</p>
</div>
<div id="ref-Bickel2004a">
<p>Bickel, Peter J., and Elizaveta Levina. 2004. “Some theory for Fisher’s linear discriminant function, ‘naive Bayes’, and some alternatives when there are many more variables than observations.” <em>Bernoulli</em> 10 (6). Bernoulli Society for Mathematical Statistics; Probability: 989–1010. <a href="http://projecteuclid.org/euclid.bj/1106314847" class="uri">http://projecteuclid.org/euclid.bj/1106314847</a>.</p>
</div>
<div id="ref-Bishop2006a">
<p>Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. Information Science and Statistics. Springer. <a href="http://doi.org/10.1117/1.2819119">doi:10.1117/1.2819119</a>.</p>
</div>
<div id="ref-Bishop1998a">
<p>Bishop, Christopher M., Markus Svensén, and Christopher K. I. Williams. 1998. “GTM: The Generative Topographic Mapping.” <em>Neural Computation</em> 10 (1). MIT Press 238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu: 215–34. <a href="http://doi.org/10.1162/089976698300017953">doi:10.1162/089976698300017953</a>.</p>
</div>
<div id="ref-Bouveyron2007">
<p>Bouveyron, Charles, S. Girard, and C. Schmid. 2007. “High-dimensional data clustering.” <em>Computational Statistics &amp; Data Analysis</em> 52 (1): 502–19. <a href="http://doi.org/10.1016/j.csda.2007.02.009">doi:10.1016/j.csda.2007.02.009</a>.</p>
</div>
<div id="ref-Breiman2001a">
<p>Breiman, Leo. 2001. “Random forests.” <em>Machine Learning</em> 45 (1). Springer: 5–32.</p>
</div>
<div id="ref-Candes2006a">
<p>Candès, Emmanuel J. 2006. “Compressive sampling.” <em>Proceedings of the International Congress of Mathematicians, Madrid, Spain</em> 3: 1433–52.</p>
</div>
<div id="ref-Candes2009b">
<p>Candès, Emmanuel J., Xiaodong Li, Yi Ma, and John Wright. 2009. “Robust Principal Component Analysis?” <em>Journal of the ACM</em> 58 (3). ACM: 1–37. <a href="http://doi.org/10.1145/1970392.1970395">doi:10.1145/1970392.1970395</a>.</p>
</div>
<div id="ref-Chang2011a">
<p>Chang, Chih-Chung, and Chih-Jen Lin. 2011. “LIBSVM.” <em>ACM Transactions on Intelligent Systems and Technology</em> 2 (3). ACM: 1–27. <a href="http://doi.org/10.1145/1961189.1961199">doi:10.1145/1961189.1961199</a>.</p>
</div>
<div id="ref-Clemmensen2011a">
<p>Clemmensen, Line, Trevor Hastie, Daniela M. Witten, and Bjarne Ersbøll. 2011. “Sparse Discriminant Analysis.” <em>Technometrics</em> 53 (4): 406–13. <a href="http://doi.org/10.1198/TECH.2011.08118">doi:10.1198/TECH.2011.08118</a>.</p>
</div>
<div id="ref-Coifman2006a">
<p>Coifman, Ronald R, and S Lafon. 2006. “Diffusion maps.” <em>Applied and Computational Harmonic Analysis</em> 21 (1): 5–30. <a href="http://doi.org/10.1016/j.acha.2006.04.006">doi:10.1016/j.acha.2006.04.006</a>.</p>
</div>
<div id="ref-Cook2005a">
<p>Cook, R. Dennis, and Liqiang Ni. 2005. “Sufficient Dimension Reduction via Inverse Regression.” <em>Journal of the American Statistical Association</em> 100 (470). Taylor &amp; Francis: 410–28. <a href="http://doi.org/10.1198/016214504000001501">doi:10.1198/016214504000001501</a>.</p>
</div>
<div id="ref-Cook2013">
<p>Cook, R. Dennis, Liliana Forzani, and Adam J. Rothman. 2013. “Prediction in abundant high-dimensional linear regression.” <em>Electronic Journal of Statistics</em> 7: 3059–88. <a href="https://projecteuclid.org/euclid.ejs/1387207935" class="uri">https://projecteuclid.org/euclid.ejs/1387207935</a>.</p>
</div>
<div id="ref-Donoho2008a">
<p>Donoho, David L., and Jiashun Jin. 2008. “Higher criticism thresholding: Optimal feature selection when useful features are rare and weak.” <em>Proceedings of the National Academy of Sciences of the United States of America</em> 105 (39): 14790–5. <a href="http://doi.org/10.1073/pnas.0807471105">doi:10.1073/pnas.0807471105</a>.</p>
</div>
<div id="ref-Eckart1936a">
<p>Eckart, Carl, and Gale Young. 1936. “The approximation of one matrix by another of lower rank.” <em>Psychometrika</em> 1 (3). Springer New York: 211–18. <a href="http://doi.org/10.1007/BF02288367">doi:10.1007/BF02288367</a>.</p>
</div>
<div id="ref-Eklund2012">
<p>Eklund, Anders, Mats Andersson, Camilla Josephson, Magnus Johannesson, and Hans Knutsson. 2012. “Does parametric fMRI analysis with SPM yield valid results?-An empirical study of 1484 rest datasets.” <em>NeuroImage</em> 61 (3). Elsevier Inc.: 565–78. <a href="http://doi.org/10.1016/j.neuroimage.2012.03.093">doi:10.1016/j.neuroimage.2012.03.093</a>.</p>
</div>
<div id="ref-Fan2008a">
<p>Fan, Jianqing, and Yingying Fan. 2008. “High-dimensional classification using features annealed independence rules.” <em>The Annals of Statistics</em> 36 (6). Institute of Mathematical Statistics: 2605–37. <a href="http://projecteuclid.org/euclid.aos/1231165181" class="uri">http://projecteuclid.org/euclid.aos/1231165181</a>.</p>
</div>
<div id="ref-Fan2012a">
<p>Fan, Jianqing, Yang Feng, and Xin Tong. 2012. “A road to classification in high dimensional space: the regularized optimal affine discriminant.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 74 (4): 745–71. <a href="http://doi.org/10.1111/j.1467-9868.2012.01029.x">doi:10.1111/j.1467-9868.2012.01029.x</a>.</p>
</div>
<div id="ref-Fisher1925a">
<p>Fisher, R. A. 1925. “Theory of Statistical Estimation.” <em>Mathematical Proceedings of the Cambridge Philosophical Society</em> 22 (05). Cambridge University Press: 700–725. <a href="http://doi.org/10.1017/S0305004100009580">doi:10.1017/S0305004100009580</a>.</p>
</div>
<div id="ref-Friedman1989a">
<p>Friedman, Jerome H. 1989. “Regularized Discriminant Analysis.” <em>Journal of the American Statistical Association</em> 84 (405). Taylor &amp; Francis: 165–75. <a href="http://doi.org/10.1080/01621459.1989.10478752">doi:10.1080/01621459.1989.10478752</a>.</p>
</div>
<div id="ref-Fukumizu2004a">
<p>Fukumizu, Kenji, Francis R Bach, and Michael I. Jordan. 2004. “Dimensionality Reduction for Supervised Learning with Reproducing Kernel Hilbert Spaces.” <em>Journal of Machine Learning Research</em> 5: 73–99.</p>
</div>
<div id="ref-Globerson2003a">
<p>Globerson, Amir, and Naftali Tishby. 2003. “Sufficient Dimensionality Reduction.” Edited by Isabelle Guyon and Andre Elisseeff. <em>Journal of Machine Learning Research</em> 3 (7-8): 1307–31. <a href="http://doi.org/10.1162/153244303322753689">doi:10.1162/153244303322753689</a>.</p>
</div>
<div id="ref-Halko2011a">
<p>Halko, N., P. G. Martinsson, and Joel A. Tropp. 2011. “Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions.” <em>SIAM Review</em> 53 (2). Society for Industrial; Applied Mathematics: 217–88. <a href="http://doi.org/10.1137/090771806">doi:10.1137/090771806</a>.</p>
</div>
<div id="ref-Hastie2004">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2004. “The Elements of Statistical Learning: Data Mining, Inference, and Prediction.” <em>BeiJing: Publishing House of Electronics Industry</em>.</p>
</div>
<div id="ref-Householder1938">
<p>Householder, A S, and Gale Young. 1938. “Discussion of a set of points in terms of their mutual distances.” <em>Psychometrika</em> 3 (1). Springer: 19–22. <a href="http://doi.org/10.1007/BF02287916">doi:10.1007/BF02287916</a>.</p>
</div>
<div id="ref-Huber1981a">
<p>Huber, Peter J. 1981. <em>Robust Statistics</em>. Vol. 82. Wiley Series in Probability and Statistics 3. Wiley. <a href="http://doi.org/10.1002/9780470434697">doi:10.1002/9780470434697</a>.</p>
</div>
<div id="ref-Huber1985a">
<p>———. 1985. “Projection Pursuit.” <em>The Annals of Statistics</em> 13 (2). Institute of Mathematical Statistics: 435–75. <a href="http://projecteuclid.org/euclid.aos/1176349519" class="uri">http://projecteuclid.org/euclid.aos/1176349519</a>.</p>
</div>
<div id="ref-Jolliffe2002a">
<p>Jolliffe, I T. 2002. <em>Principal Component Analysis</em>. Vol. 98. <a href="http://doi.org/10.1007/b98835">doi:10.1007/b98835</a>.</p>
</div>
<div id="ref-Kohonen1982a">
<p>Kohonen, Teuvo. 1982. “Self-organized formation of topologically correct feature maps.” <em>Biological Cybernetics</em> 43 (1): 59–69. <a href="http://doi.org/10.1007/BF00337288">doi:10.1007/BF00337288</a>.</p>
</div>
<div id="ref-mnist">
<p>LeCun, Yann, Corinna Cortes, and Chris Burges. 2015. “MNIST handwritten digit database.” Accessed July 1. <a href="http://yann.lecun.com/exdb/mnist/" class="uri">http://yann.lecun.com/exdb/mnist/</a>.</p>
</div>
<div id="ref-Li1991a">
<p>Li, Ker-Chau. 1991. “Sliced Inverse Regression for Dimension Reduction.” <em>Journal of the American Statistical Association</em> 86 (414). Taylor &amp; Francis: 316–27. <a href="http://doi.org/10.1080/01621459.1991.10475035">doi:10.1080/01621459.1991.10475035</a>.</p>
</div>
<div id="ref-Lopes2011a">
<p>Lopes, Miles, Laurent Jacob, and Martin J. Wainwright. 2011. “A More Powerful Two-Sample Test in High Dimensions using Random Projection.” In <em>Neural Information Processing Systems</em>, 1206–14. <a href="http://papers.nips.cc/paper/4260-a-more-powerful-two-sample-test-in-high-dimensions-using-random-projection" class="uri">http://papers.nips.cc/paper/4260-a-more-powerful-two-sample-test-in-high-dimensions-using-random-projection</a>.</p>
</div>
<div id="ref-Mai2013a">
<p>Mai, Qing, and Hui Zou. 2013. “A Note On the Connection and Equivalence of Three Sparse Linear Discriminant Analysis Methods.” <em>Technometrics</em> 55: 243–46. <a href="http://doi.org/10.1080/00401706.2012.746208">doi:10.1080/00401706.2012.746208</a>.</p>
</div>
<div id="ref-Mairal2009">
<p>Mairal, Julien, Jean Ponce, Guillermo Sapiro, Andrew Zisserman, and Francis R. Bach. 2009. “Supervised Dictionary Learning.” In <em>Advances in Neural Information Processing Systems</em>, 1033–40. <a href="http://papers.nips.cc/paper/3448-supervised" class="uri">http://papers.nips.cc/paper/3448-supervised</a>.</p>
</div>
<div id="ref-Mika1999a">
<p>Mika, S., G. Ratsch, J. Weston, B. Scholkopf, and K.R. Mullers. 1999. “Fisher discriminant analysis with kernels.” In <em>Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468)</em>, 41–48. IEEE. <a href="http://doi.org/10.1109/NNSP.1999.788121">doi:10.1109/NNSP.1999.788121</a>.</p>
</div>
<div id="ref-Olshausen1997a">
<p>Olshausen, Bruno A., and David J. Field. 1997. “Sparse coding with an overcomplete basis set: A strategy employed by V1?” <em>Vision Research</em> 37 (23): 3311–25. <a href="http://doi.org/10.1016/S0042-6989(97)00169-7">doi:10.1016/S0042-6989(97)00169-7</a>.</p>
</div>
<div id="ref-Paul2007a">
<p>Paul, Debashis. 2007. “Asymptotics of sample eigenstructure for a large dimensional spiked covariance model.” <em>Statistica Sinica</em> 17 (4): 1617. <a href="http://www3.stat.sinica.edu.tw/statistica/oldpdf/A17n418.pdf" class="uri">http://www3.stat.sinica.edu.tw/statistica/oldpdf/A17n418.pdf</a>.</p>
</div>
<div id="ref-Pearson1901a">
<p>Pearson, Karl. 1901. “On lines and planes of closest fit to systems of points in space.” <em>Philosophical Magazine Series 6</em> 2 (11). Taylor &amp; Francis Group: 559–72. <a href="http://doi.org/10.1080/14786440109462720">doi:10.1080/14786440109462720</a>.</p>
</div>
<div id="ref-Rousseeuw1999a">
<p>Rousseeuw, Peter J., and Katrien Van Driessen. 1999. “A Fast Algorithm for the Minimum Covariance Determinant Estimator.” <em>Technometrics</em> 41 (3). Taylor &amp; Francis Group: 212–23. <a href="http://amstat.tandfonline.com/doi/abs/10.1080/00401706.1999.10485670\#.VTejtq1Viko" class="uri">http://amstat.tandfonline.com/doi/abs/10.1080/00401706.1999.10485670\#.VTejtq1Viko</a>.</p>
</div>
<div id="ref-Roweis2000a">
<p>Roweis, Sam T, and L K Saul. 2000. “Nonlinear dimensionality reduction by locally linear embedding.” <em>Science (New York, N.Y.)</em> 290 (5500): 2323–6. <a href="http://doi.org/10.1126/science.290.5500.2323">doi:10.1126/science.290.5500.2323</a>.</p>
</div>
<div id="ref-deSilva2003">
<p>Silva, V de, and Joshua B Tenenbaum. 2003. “Global Versus Local Methods in Nonlinear Dimensionality Reduction.” In <em>Neural Information Processing Systems</em>, 721–28.</p>
</div>
<div id="ref-Tenenbaum2000a">
<p>Tenenbaum, Joshua B, V de Silva, John C Langford, and Vin De Silva. 2000. “A global geometric framework for nonlinear dimensionality reduction.” <em>Science</em> 290 (5500): 2319–23. <a href="http://doi.org/10.1126/science.290.5500.2319">doi:10.1126/science.290.5500.2319</a>.</p>
</div>
<div id="ref-Tibshirani1996">
<p>Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” <em>Journal of the Royal Statistical Society. Series B</em> 58: 267–88.</p>
</div>
<div id="ref-Tibshirani2002a">
<p>Tibshirani, Robert, Trevor Hastie, Balasubramanian Narasimhan, and Gilbert Chu. 2002. “Diagnosis of multiple cancer types by shrunken centroids of gene expression.” <em>Proceedings of the National Academy of Sciences of the United States of America</em> 99 (10): 6567–72. <a href="http://doi.org/10.1073/pnas.082099299">doi:10.1073/pnas.082099299</a>.</p>
</div>
<div id="ref-Tishby1999a">
<p>Tishby, Naftali, Fernando C Pereira, and William Bialek. 1999. “The information bottleneck method arXiv : physics / 0004057v1 [ physics . data-an ] 24 Apr 2000.” <em>Neural Computation</em>, 1–16.</p>
</div>
<div id="ref-Trunk1979a">
<p>Trunk, G V. 1979. “A problem of dimensionality: a simple example.” <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 1 (3). Naval Research Laboratory, Washington, DC 20375. New York, U.S.A.: 306–7. <a href="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4766926" class="uri">http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4766926</a>.</p>
</div>
<div id="ref-Witten2009a">
<p>Witten, Daniela M., and Robert Tibshirani. 2009. “Covariance-regularized regression and classification for high-dimensional problems.” <em>Journal of the Royal Statistical Society. Series B, Statistical Methodology</em> 71 (3): 615–36. <a href="http://doi.org/10.1111/j.1467-9868.2009.00699.x">doi:10.1111/j.1467-9868.2009.00699.x</a>.</p>
</div>
<div id="ref-Zhang2014a">
<p>Zhang, Teng, and Gilad Lerman. 2014. “A Novel M-Estimator for Robust PCA.” <em>Journal of Machine Learning Research</em> 15: 749–808. <a href="http://jmlr.org/papers/v15/zhang14a.html" class="uri">http://jmlr.org/papers/v15/zhang14a.html</a>.</p>
</div>
<div id="ref-Zhang2004f">
<p>Zhang, Zhenyue, and Hongyuan Zha. 2004. “Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment.” <em>SIAM Journal on Scientific Computing</em> 26 (1). Society for Industrial; Applied Mathematics: 313–38. <a href="http://doi.org/10.1137/S1064827502419154">doi:10.1137/S1064827502419154</a>.</p>
</div>
<div id="ref-Zou2006a">
<p>Zou, Hui. 2006. “The Adaptive Lasso and Its Oracle Properties.” <a href="http://doi.org/10.1198/016214506000000735">doi:10.1198/016214506000000735</a>.</p>
</div>
</div>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Although  is not a 2-step method (where embedding is learned first, and then a classifier is applied), adaptive lasso <span class="citation" data-cites="Zou2006a">Zou (<a href="#ref-Zou2006a">2006</a>)</span> and its variants improve on lasso’s theoretical and empirical properties, so we consider such an approach here.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>When having to estimate the eigenvector from the data,  performs even worse. This is because when <span class="math inline">\(n \ll p\)</span>,  is an inconsistent estimator with large variance <span class="citation" data-cites="Baik2006a Paul2007a">Baik and Silverstein (<a href="#ref-Baik2006a">2006</a>; Paul <a href="#ref-Paul2007a">2007</a>)</span><a href="#fnref2">↩</a></p></li>
</ol>
</section>
</body>
</html>
