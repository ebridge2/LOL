\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{TBF01}
\citation{piMartino2014}
\citation{Eklund2012a}
\citation{Friedman1989,Bickel2004,Bouveyron07a}
\citation{Witten09a}
\citation{Olshausen1997}
\citation{Tibs96}
\citation{Buhlmann2011}
\citation{Tibs96}
\citation{ponoho08a}
\citation{Tibshirani2002,Fan2008,Witten09a,Clemmensen2011,Mai2013,Fan2012}
\citation{Young1938,Borg2010}
\citation{Pearson1901,Jolliffe2002}
\citation{Kohonen1982}
\citation{Bishop1998}
\citation{Tenenbaum2000}
\citation{Roweis2000}
\citation{Belkin2003}
\citation{Zhang2004b}
\citation{Coifman2006}
\citation{Allard12a}
\citation{Belhumeur1997}
\@writefile{brf}{\backcite{TBF01}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Eklund2012a}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Friedman1989}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Bouveyron07a}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Witten09a}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Olshausen1997}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Tibs96}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Buhlmann2011}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Tibs96}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Witten09a}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Tibshirani2002}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Fan2012}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Young1938}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Borg2010}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Pearson1901}{{1}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Jolliffe2002}{{1}{(document)}{Doc-Start}}}
\citation{Eckart1936,deSilva2003,Allard12a}
\citation{Li1991,TB99,Globerson2003,Cook2005,Fukumizu2004}
\citation{Cook13a}
\citation{Huber1985}
\citation{Belkin2006}
\citation{Mairal2008}
\citation{Belkin2006}
\citation{Bouveyron07a}
\citation{Zou2006a}
\citation{mnist}
\@writefile{brf}{\backcite{Kohonen1982}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Bishop1998}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Tenenbaum2000}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Roweis2000}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Belkin2003}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Zhang2004b}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Coifman2006}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Allard12a}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Belhumeur1997}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Allard12a}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Eckart1936}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{deSilva2003}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Li1991}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Fukumizu2004}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Cook13a}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Huber1985}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Belkin2006}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Mairal2008}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Belkin2006}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Bouveyron07a}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{mnist}{{2}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Zou2006a}{{2}{1}{Hfootnote.1}}}
\citation{Bickel2004}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Illustrating three different classifiers---{\sc  \texttt  {Lasso}}\nobreakspace  {}(top), {\sc  \texttt  {Fld$\circ $Pca}}\nobreakspace  {}(middle), and {\sc  \texttt  {Lol}}\nobreakspace  {}(bottom)---for embedding images of the digits 3, 7, and 8 (from MNIST), each of which is 28 $\times $ 28 = 784 dimensional. \textbf  {(A)}: Exemplars, boundary colors are only for visualization purposes. \textbf  {(B)}: The first four projection matrices learned by the three different approaches on 300 training samples. Note that {\sc  \texttt  {Lasso}}\nobreakspace  {}is sparse and supervised, {\sc  \texttt  {Pca}}\nobreakspace  {}is dense and unsupervised, and {\sc  \texttt  {Lol}}\nobreakspace  {}is dense and supervised. \textbf  {(C)}: Embedding 500 test samples into the top 2 dimensions using each approach. Digits color coded as in (A). \textbf  {(D)}: The estimated posterior distribution of test samples after 5-dimensional projection learned via each method. We show only 3 vs. 8 for simplicity. The vertical line shows the classification threshold. The filled area is the estimated error rate: the goal of any classification algorithm is to minimize that area. Clearly, {\sc  \texttt  {Lol}}\nobreakspace  {}exhibits the best separation after embedding, which results in the best classification performance. \relax }}{3}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mnist}{{1}{3}{Illustrating three different classifiers---\sct {Lasso}~(top), \sct {Fld$\circ $Pca}~(middle), and \Lol ~(bottom)---for embedding images of the digits 3, 7, and 8 (from MNIST), each of which is 28 $\times $ 28 = 784 dimensional. \textbf {(A)}: Exemplars, boundary colors are only for visualization purposes. \textbf {(B)}: The first four projection matrices learned by the three different approaches on 300 training samples. Note that \sct {Lasso}~is sparse and supervised, \Pca ~is dense and unsupervised, and \Lol ~is dense and supervised. \textbf {(C)}: Embedding 500 test samples into the top 2 dimensions using each approach. Digits color coded as in (A). \textbf {(D)}: The estimated posterior distribution of test samples after 5-dimensional projection learned via each method. We show only 3 vs. 8 for simplicity. The vertical line shows the classification threshold. The filled area is the estimated error rate: the goal of any classification algorithm is to minimize that area. Clearly, \Lol ~exhibits the best separation after embedding, which results in the best classification performance. \relax }{figure.caption.3}{}}
\citation{Fan2012}
\citation{Fan2012}
\citation{Baik2006,Paul2007}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  {\sc  \texttt  {Lol}}\nobreakspace  {}achieves near optimal performance for a wide variety of distributions. Each point is sampled from a multivariate Gaussian; the three columns correspond to different simulation parameters (see Methods for details). In each of 3 simulations, we sample n=100 points in p=1000 dimensions. And for each approach, we embed into the top 20 dimensions. Note that we use the sample estimates, rather than the true population values of the parameters. In this setting, the results are similar. \textbf  {(A)}: The mean difference vector is aligned with the direction of maximal variance, maxing it ideal for both {\sc  \texttt  {Pca}}\nobreakspace  {}to discover the discriminant direction and a sparse solution. \textbf  {(B)}: The mean difference vector is orthogonal to the direction of maximal variance, making {\sc  \texttt  {Pca}}\nobreakspace  {}fail, but sparse methods can still recover the correct dimensions. \textbf  {(C)}: Same as (B), but the data are rotated. \textbf  {Row 1}: A scatter plot of the first two dimensions of the sampled points, with class 0 and 1 as black and gray dots, respectively. \textbf  {Row 2}: {\sc  \texttt  {Fld $\circ $ Pca}}. \textbf  {Row 3}: {\sc  \texttt  {Road}}, a sparse method designed specifically for this model \cite  {Fan2012}. \textbf  {Row 4}: {\sc  \texttt  {Lol}}, our newly proposed method. \textbf  {Row 5}: the Bayes optimal classifier, which is what all classifiers strive to achieve. Note that {\sc  \texttt  {Lol}}\nobreakspace  {}is closest to Bayes optimal in all three settings. \relax }}{4}{figure.caption.5}}
\@writefile{brf}{\backcite{Fan2012}{{4}{2}{figure.caption.5}}}
\newlabel{fig:cigars}{{2}{4}{\Lol ~achieves near optimal performance for a wide variety of distributions. Each point is sampled from a multivariate Gaussian; the three columns correspond to different simulation parameters (see Methods for details). In each of 3 simulations, we sample n=100 points in p=1000 dimensions. And for each approach, we embed into the top 20 dimensions. Note that we use the sample estimates, rather than the true population values of the parameters. In this setting, the results are similar. \textbf {(A)}: The mean difference vector is aligned with the direction of maximal variance, maxing it ideal for both \Pca ~to discover the discriminant direction and a sparse solution. \textbf {(B)}: The mean difference vector is orthogonal to the direction of maximal variance, making \Pca ~fail, but sparse methods can still recover the correct dimensions. \textbf {(C)}: Same as (B), but the data are rotated. \textbf {Row 1}: A scatter plot of the first two dimensions of the sampled points, with class 0 and 1 as black and gray dots, respectively. \textbf {Row 2}: \sct {Fld $\circ $ Pca}. \textbf {Row 3}: \sct {Road}, a sparse method designed specifically for this model \cite {Fan2012}. \textbf {Row 4}: \Lol , our newly proposed method. \textbf {Row 5}: the Bayes optimal classifier, which is what all classifiers strive to achieve. Note that \Lol ~is closest to Bayes optimal in all three settings. \relax }{figure.caption.5}{}}
\@writefile{brf}{\backcite{Bickel2004}{{4}{(document)}{Doc-Start}}}
\citation{Trunk1979}
\newlabel{thm:LDA}{{1}{5}{}{thm.1}{}}
\@writefile{brf}{\backcite{Trunk1979}{{5}{(document)}{Doc-Start}}}
\newlabel{thm:n}{{2}{5}{}{thm.2}{}}
\newlabel{thm:FAT}{{3}{5}{}{thm.3}{}}
\@writefile{brf}{\backcite{Baik2006}{{5}{2}{Hfootnote.2}}}
\@writefile{brf}{\backcite{Paul2007}{{5}{2}{Hfootnote.2}}}
\citation{TBF01}
\citation{Huber1981,Rousseeuw1999,Qin2013a}
\citation{Candes2009a}
\citation{Zhang2014}
\citation{Bishop2006}
\citation{Bickel2004}
\citation{Halko2011}
\citation{Candes06a}
\@writefile{brf}{\backcite{TBF01}{{6}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Huber1981}{{6}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Qin2013a}{{6}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Candes2009a}{{6}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Zhang2014}{{6}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Bishop2006}{{6}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Bickel2004}{{6}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Halko2011}{{6}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Candes06a}{{6}{(document)}{Doc-Start}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Seven simulations demonstrating that even when the true discriminant boundary is high-dimensional, {\sc  \texttt  {Lol}}\nobreakspace  {}can find a low-dimensional projection that wins the bias-variance trade-off against competing methods. For the first four, the top panels depict the means (top), the shared covariance matrix (middle). For the next three, the top panels depict a 2D scatter plot (left), mean and level set of one standard deviation of covariance matrix (right). For all seven simulations, the bottom panel shows misclassification rate as a function of the number of embedded dimensions, for several different classifiers. The simulations settings are as follows: \textbf  {(A)} Rotated Trunk: same as Figure \ref  {fig:cigars}(C). \textbf  {(B)} Toeplitz: another setting where mean difference is not well correlated with any eigenvector, and no ambient coordinate is particularly useful on its own. \textbf  {(C)} Fat Tails: a common phenomenon in real data; we have theory to support this generalization of the LDA model. \textbf  {(D)} 3 Classes: {\sc  \texttt  {Lol}}\nobreakspace  {}naturally adapts to multiple classes. \textbf  {(E)} QDA: QOQ, a variant of {\sc  \texttt  {Lol}}\nobreakspace  {}when each class has a unique covariance, outperforms {\sc  \texttt  {Lol}}, as expected. \textbf  {(F)} Outliers: adding high-dimensional outliers degrades performance of standard eigensolvers, but those can easily be replaced in {\sc  \texttt  {Lol}}\nobreakspace  {}for a robust variants (called {\sc  \texttt  {Lrl}}). \textbf  {(F)} XOR: a high-dimensional stochastic generalization of XOR, demonstrating the {\sc  \texttt  {Lol}}\nobreakspace  {}and QOQ work even in scenarios that are quite distinct from the original motivating problems. In all 7 cases, {\sc  \texttt  {Lol}}, or the appropriate generalization thereof, outperforms unsupervised, sparse, or other methods. Moreover, the optimal embedding dimension is never the true discriminant dimension, but rather, a smaller number jointly determined by parameter settings and sample size. \relax }}{7}{figure.caption.9}}
\newlabel{fig:properties}{{3}{7}{Seven simulations demonstrating that even when the true discriminant boundary is high-dimensional, \Lol ~can find a low-dimensional projection that wins the bias-variance trade-off against competing methods. For the first four, the top panels depict the means (top), the shared covariance matrix (middle). For the next three, the top panels depict a 2D scatter plot (left), mean and level set of one standard deviation of covariance matrix (right). For all seven simulations, the bottom panel shows misclassification rate as a function of the number of embedded dimensions, for several different classifiers. The simulations settings are as follows: \textbf {(A)} Rotated Trunk: same as Figure \ref {fig:cigars}(C). \textbf {(B)} Toeplitz: another setting where mean difference is not well correlated with any eigenvector, and no ambient coordinate is particularly useful on its own. \textbf {(C)} Fat Tails: a common phenomenon in real data; we have theory to support this generalization of the LDA model. \textbf {(D)} 3 Classes: \Lol ~naturally adapts to multiple classes. \textbf {(E)} QDA: QOQ, a variant of \Lol ~when each class has a unique covariance, outperforms \Lol , as expected. \textbf {(F)} Outliers: adding high-dimensional outliers degrades performance of standard eigensolvers, but those can easily be replaced in \Lol ~for a robust variants (called \Lrl ). \textbf {(F)} XOR: a high-dimensional stochastic generalization of XOR, demonstrating the \Lol ~and QOQ work even in scenarios that are quite distinct from the original motivating problems. In all 7 cases, \Lol , or the appropriate generalization thereof, outperforms unsupervised, sparse, or other methods. Moreover, the optimal embedding dimension is never the true discriminant dimension, but rather, a smaller number jointly determined by parameter settings and sample size. \relax }{figure.caption.9}{}}
\citation{Lopes2011}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Computational efficiency of various low-dimensional projection methods. In all cases, n=100, and we used the ``stacked cigars'' simulation parameters. We compare {\sc  \texttt  {Pca}}\nobreakspace  {}with the projection steps from {\sc  \texttt  {Lol}}, QOQ, {\sc  \texttt  {Lrl}}, {\sc  \texttt  {Lfl}}, and {\sc  \texttt  {Lal}}, for different values of (p,d). The addition of the mean difference vector is essentially negligible. Moreover, for small d, the {\sc  \texttt  {Lfl}}\nobreakspace  {}is advantageous. {\sc  \texttt  {Lal}}\nobreakspace  {}is always fastest, and its performance is often comparable to other methods (not shown). \relax }}{8}{figure.caption.12}}
\newlabel{fig:speed}{{4}{8}{Computational efficiency of various low-dimensional projection methods. In all cases, n=100, and we used the ``stacked cigars'' simulation parameters. We compare \Pca ~with the projection steps from \Lol , QOQ, \Lrl , \Lfl , and \Lal , for different values of (p,d). The addition of the mean difference vector is essentially negligible. Moreover, for small d, the \Lfl ~is advantageous. \Lal ~is always fastest, and its performance is often comparable to other methods (not shown). \relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  For four standard datasets, we benchmark {\sc  \texttt  {Lol}}\nobreakspace  {}(green circles) versus standard classification methods, including support vector machines (blue up triangles), {\sc  \texttt  {Road}} (cyan down triangles), {\sc  \texttt  {Lasso}}\nobreakspace  {}(magenta pluses), and random forest (orange diamonds). Top panels show error rate as a function of log$_2$ number of embedded dimensions (for {\sc  \texttt  {Lol}}, {\sc  \texttt  {Road}}, and {\sc  \texttt  {Lasso}}) or cost (for SVM). Bottom panels show the minimum error rate achieved by each of the five algorithms versus time. The lower left dark gray (upper right light gray) rectangle is the area in which any algorithm is \emph  {better} (worse) than {\sc  \texttt  {Lol}}\nobreakspace  {}in terms of both accuracy and efficiency. \textbf  {(A)} Prostate: a standard sparse dataset. 1-dimensional {\sc  \texttt  {Lol}}\nobreakspace  {}does very well, although keeping $2^5$ ambient coordinates slightly improves performance, at a significant cost of compute time (two orders of magnitude), with minimal additional interpretability. \textbf  {(B)} Colon: another standard sparse dataset. Here, 2-4 dimensions of {\sc  \texttt  {Lol}}\nobreakspace  {}outperforms all other approaches considered. \textbf  {(C)} MNIST: 10 image categories here, so {\sc  \texttt  {Road}} is not possible. {\sc  \texttt  {Lol}}\nobreakspace  {}does very well regardless of the number of dimensions kept. SVN marginally improves on {\sc  \texttt  {Lol}}\nobreakspace  {}accuracy, at a significant cost in computation (two orders of magnitude). \textbf  {(D)} CIFAR-10: a higher dimensional and newer 10 category image classification problem. Results are qualitatively similar to (C). Note that, for none of the problems is there an algorithm ever performing better and faster than {\sc  \texttt  {Lol}}; rather, most algorithms typically perform worse and slower (though some are more accurate and much more computationally expensive. This suggests that regardless of how one subjectively weights computational efficiency versus accuracy, {\sc  \texttt  {Lol}}\nobreakspace  {}is the best default algorithm in a variety of real data settings. \relax }}{9}{figure.caption.14}}
\newlabel{fig:realdata}{{5}{9}{For four standard datasets, we benchmark \Lol ~(green circles) versus standard classification methods, including support vector machines (blue up triangles), \sct {Road} (cyan down triangles), \sct {Lasso}~(magenta pluses), and random forest (orange diamonds). Top panels show error rate as a function of log$_2$ number of embedded dimensions (for \Lol , \sct {Road}, and \sct {Lasso}) or cost (for SVM). Bottom panels show the minimum error rate achieved by each of the five algorithms versus time. The lower left dark gray (upper right light gray) rectangle is the area in which any algorithm is \emph {better} (worse) than \Lol ~in terms of both accuracy and efficiency. \textbf {(A)} Prostate: a standard sparse dataset. 1-dimensional \Lol ~does very well, although keeping $2^5$ ambient coordinates slightly improves performance, at a significant cost of compute time (two orders of magnitude), with minimal additional interpretability. \textbf {(B)} Colon: another standard sparse dataset. Here, 2-4 dimensions of \Lol ~outperforms all other approaches considered. \textbf {(C)} MNIST: 10 image categories here, so \sct {Road} is not possible. \Lol ~does very well regardless of the number of dimensions kept. SVN marginally improves on \Lol ~accuracy, at a significant cost in computation (two orders of magnitude). \textbf {(D)} CIFAR-10: a higher dimensional and newer 10 category image classification problem. Results are qualitatively similar to (C). Note that, for none of the problems is there an algorithm ever performing better and faster than \Lol ; rather, most algorithms typically perform worse and slower (though some are more accurate and much more computationally expensive. This suggests that regardless of how one subjectively weights computational efficiency versus accuracy, \Lol ~is the best default algorithm in a variety of real data settings. \relax }{figure.caption.14}{}}
\citation{Lopes2011}
\citation{Lopes2011}
\citation{Belhumeur1997}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  The intuition of including the mean difference vector is equally useful for other supervised manifold learning problems, including testing and regression. (A) and (B) show two different high-dimensional testing settings, as described in Methods. Power is plotted against the decay rate of the spectrum, which approximates the effective number of dimensions. {\sc  \texttt  {Lol}}\nobreakspace  {}composed with Hotelling's test outperforms the random projections variants described in \cite  {Lopes2011}, as well as several other variants. (C) shows a high-dimensional regression settings, as described in Methods. Log$_{10}$ mean squared error is plotted against the number of embedded dimensions. Regression {\sc  \texttt  {Lol}}\nobreakspace  {}composed with linear regression outperforms {\sc  \texttt  {Lasso}}\nobreakspace  {}(cyan), the classic sparse regression method, as well as partial least squares (PLS; black). In the legend, 'A' denote either 'linear regression' (in (C)), or 'Hotelling' (in (A) and (B)). These three simulation settings therefore demonstrate the generality of this technique. \relax }}{10}{figure.caption.16}}
\@writefile{brf}{\backcite{Lopes2011}{{10}{6}{figure.caption.16}}}
\newlabel{fig:generalizations}{{6}{10}{The intuition of including the mean difference vector is equally useful for other supervised manifold learning problems, including testing and regression. (A) and (B) show two different high-dimensional testing settings, as described in Methods. Power is plotted against the decay rate of the spectrum, which approximates the effective number of dimensions. \Lol ~composed with Hotelling's test outperforms the random projections variants described in \cite {Lopes2011}, as well as several other variants. (C) shows a high-dimensional regression settings, as described in Methods. Log$_{10}$ mean squared error is plotted against the number of embedded dimensions. Regression \Lol ~composed with linear regression outperforms \sct {Lasso}~(cyan), the classic sparse regression method, as well as partial least squares (PLS; black). In the legend, 'A' denote either 'linear regression' (in (C)), or 'Hotelling' (in (A) and (B)). These three simulation settings therefore demonstrate the generality of this technique. \relax }{figure.caption.16}{}}
\@writefile{brf}{\backcite{Lopes2011}{{10}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Belhumeur1997}{{10}{(document)}{Doc-Start}}}
\citation{Mika1999}
\citation{Breiman2001}
\citation{Allard12a}
\citation{Chang2011}
\@writefile{brf}{\backcite{Mika1999}{{11}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Breiman2001}{{11}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Allard12a}{{11}{(document)}{Doc-Start}}}
\@writefile{brf}{\backcite{Chang2011}{{11}{(document)}{Doc-Start}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Table of algorithms and their properties for high-dimensional data. Gray elements indicate that results are demonstrated in the Figure labeled in the bottom row. 'X' denotes relatively good performance for a given setting, or has the particular property. \relax }}{12}{figure.caption.18}}
\newlabel{fig:table}{{7}{12}{Table of algorithms and their properties for high-dimensional data. Gray elements indicate that results are demonstrated in the Figure labeled in the bottom row. 'X' denotes relatively good performance for a given setting, or has the particular property. \relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Theory}{13}{appendix.A}}
\@writefile{toc}{\contentsline {subsection}{\numberline {I.A}The Classification Problem}{13}{subsection.A.1}}
\newlabel{eq:R}{{1}{13}{The Classification Problem}{equation.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {I.B}Linear Discriminant Analysis (LDA) Model}{13}{subsection.A.2}}
\citation{not sure who}
\citation{Anderson1999}
\@writefile{toc}{\contentsline {subsection}{\numberline {I.C}Projection Based Classifiers}{14}{subsection.A.3}}
\newlabel{eq:g_A}{{2}{14}{Projection Based Classifiers}{equation.A.2}{}}
\newlabel{eq:A}{{3}{14}{Projection Based Classifiers}{equation.A.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {I.C(1)}Bayes Optimal Projection}{14}{subsubsection.A.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {I.C(2)}Principle Components Analysis ({\sc  \texttt  {Pca}}) Projection}{14}{subsubsection.A.3.2}}
\citation{Breiman01b}
\@writefile{brf}{\backcite{Anderson1999}{{15}{I.C(2)}{subsubsection.A.3.2}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {I.C(3)}Linear Optimal Low-Rank ({\sc  \texttt  {Lol}}) Projection}{15}{subsubsection.A.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {I.C(4)}{\sc  \texttt  {Fld}}\nobreakspace  {}is rotationally invariant}{15}{subsubsection.A.3.4}}
\@writefile{brf}{\backcite{Breiman01b}{{15}{I.C(4)}{subsubsection.A.3.4}}}
\newlabel{lem:rot}{{4}{15}{}{lem.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {I.C(5)}Rotation of Projection Based Linear Classifiers $g_A$}{16}{subsubsection.A.3.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {I.C(6)}Simplifying the Objective Function}{16}{subsubsection.A.3.6}}
\newlabel{eq:angle}{{4}{16}{Simplifying the Objective Function}{equation.A.4}{}}
\newlabel{lem:angle}{{6}{16}{}{lem.6}{}}
\newlabel{cor:angle}{{7}{16}{}{lem.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {I.C(7)}Evaluating Different Projections using Eq.\nobreakspace  {}\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:angle}\unskip \@@italiccorr )}}}{17}{subsubsection.A.3.7}}
\newlabel{thm:angle_goal}{{4}{17}{}{thm.4}{}}
\newlabel{eq:angle_goal}{{5}{17}{}{equation.A.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {I.C(8)}Probabilistic Extensions of the above}{17}{subsubsection.A.3.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {I.D}Learning Classifiers from Data}{18}{subsection.A.4}}
\newlabel{eq:plugin}{{6}{18}{Learning Classifiers from Data}{equation.A.6}{}}
\bibdata{../../../../other/latex/library}
\bibcite{TBF01}{{1}{}{{}}{{}}}
\bibcite{Eklund2012a}{{2}{}{{}}{{}}}
\bibcite{Friedman1989}{{3}{}{{}}{{}}}
\bibcite{Bickel2004}{{4}{}{{}}{{}}}
\bibcite{Bouveyron07a}{{5}{}{{}}{{}}}
\bibcite{Witten09a}{{6}{}{{}}{{}}}
\bibcite{Olshausen1997}{{7}{}{{}}{{}}}
\bibcite{Tibs96}{{8}{}{{}}{{}}}
\bibcite{Buhlmann2011}{{9}{}{{}}{{}}}
\bibcite{Tibshirani2002}{{10}{}{{}}{{}}}
\bibcite{Fan2008}{{11}{}{{}}{{}}}
\bibcite{Clemmensen2011}{{12}{}{{}}{{}}}
\bibcite{Mai2013}{{13}{}{{}}{{}}}
\bibcite{Fan2012}{{14}{}{{}}{{}}}
\bibcite{Young1938}{{15}{}{{}}{{}}}
\bibcite{Borg2010}{{16}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Bibliography}{19}{appendix.B}}
\bibcite{Pearson1901}{{17}{}{{}}{{}}}
\bibcite{Jolliffe2002}{{18}{}{{}}{{}}}
\bibcite{Kohonen1982}{{19}{}{{}}{{}}}
\bibcite{Bishop1998}{{20}{}{{}}{{}}}
\bibcite{Tenenbaum2000}{{21}{}{{}}{{}}}
\bibcite{Roweis2000}{{22}{}{{}}{{}}}
\bibcite{Belkin2003}{{23}{}{{}}{{}}}
\bibcite{Zhang2004b}{{24}{}{{}}{{}}}
\bibcite{Coifman2006}{{25}{}{{}}{{}}}
\bibcite{Allard12a}{{26}{}{{}}{{}}}
\bibcite{Belhumeur1997}{{27}{}{{}}{{}}}
\bibcite{Eckart1936}{{28}{}{{}}{{}}}
\bibcite{deSilva2003}{{29}{}{{}}{{}}}
\bibcite{Li1991}{{30}{}{{}}{{}}}
\bibcite{TB99}{{31}{}{{}}{{}}}
\bibcite{Globerson2003}{{32}{}{{}}{{}}}
\bibcite{Cook2005}{{33}{}{{}}{{}}}
\bibcite{Fukumizu2004}{{34}{}{{}}{{}}}
\bibcite{Cook13a}{{35}{}{{}}{{}}}
\bibcite{Huber1985}{{36}{}{{}}{{}}}
\bibcite{Belkin2006}{{37}{}{{}}{{}}}
\bibcite{Mairal2008}{{38}{}{{}}{{}}}
\bibcite{Zou2006a}{{39}{}{{}}{{}}}
\bibcite{mnist}{{40}{}{{}}{{}}}
\bibcite{Baik2006}{{41}{}{{}}{{}}}
\bibcite{Paul2007}{{42}{}{{}}{{}}}
\bibcite{Trunk1979}{{43}{}{{}}{{}}}
\bibcite{Huber1981}{{44}{}{{}}{{}}}
\bibcite{Rousseeuw1999}{{45}{}{{}}{{}}}
\bibcite{Qin2013a}{{46}{}{{}}{{}}}
\bibcite{Candes2009a}{{47}{}{{}}{{}}}
\bibcite{Zhang2014}{{48}{}{{}}{{}}}
\bibcite{Bishop2006}{{49}{}{{}}{{}}}
\bibcite{Halko2011}{{50}{}{{}}{{}}}
\bibcite{Candes06a}{{51}{}{{}}{{}}}
\bibcite{Lopes2011}{{52}{}{{}}{{}}}
\bibcite{Mika1999}{{53}{}{{}}{{}}}
\bibcite{Breiman2001}{{54}{}{{}}{{}}}
\bibcite{Chang2011}{{55}{}{{}}{{}}}
\bibcite{Anderson1999}{{56}{}{{}}{{}}}
\bibcite{Breiman01b}{{57}{}{{}}{{}}}
\bibstyle{IEEEtran}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
